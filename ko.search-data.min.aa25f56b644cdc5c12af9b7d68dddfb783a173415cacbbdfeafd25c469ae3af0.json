[{"id":0,"href":"/2022-Q2-workshop/docs/01-overview/","title":"공분산 행렬 추정 문제","section":"2022 Q2 Bayes Workshop","content":" 1. 모형 # 1.1. 다변량 정규 모형 # $\\mu \\in \\mathbb{R}^k,~\\Sigma \\in \\mathbb{R}^{k \\times k},~\\Sigma \u0026gt; 0$에 대해 다음과 같은 모형을 생각하자1.\n$$ \\begin{equation} X_1, \\cdots, X_n | \\mu, \\Sigma \\stackrel{i.i.d.}{\\sim} N(\\mu, \\Sigma) \\end{equation} $$\n정밀도 행렬은 $\\Omega = \\Sigma^{-1}$로 정의된다.\n여기서 $\\mu$와 $\\Sigma$를 추정하는 것이 문제이다.\n일반적으로는, 평균의 추정보다 공분산 추정이 어려운데, 이는 \u0026lsquo;양의 정부호 행렬\u0026rsquo;이라는 제약이 있기 때문이다.\n1.2. 가능도 # $$ \\begin{equation} \\begin{aligned} L(\\mu, \\Sigma) \u0026amp;= \\prod_{i=1}^n N(x_i, \\mu, \\Sigma) \\\\ \u0026amp;= \\prod_{i=1}^n |2\\pi \\Sigma|^{-1/2} e^{-\\frac{1}{2}(x_i -\\mu)\u0026rsquo; \\Sigma^{-1} (x_i -\\mu)} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{1}{2}(x_i -\\mu)\u0026rsquo; \\Sigma^{-1} (x_i -\\mu)} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{1}{2} tr(\\Sigma^{-1} (x_i -\\mu) (x_i -\\mu)\u0026rsquo;} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{n}{2} tr(\\Sigma^{-1} [S_n + (\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo;])} \\end{aligned} \\end{equation} $$\n여기서 $nS_n + n(\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo; = (x_i -\\mu) (x_i -\\mu)\u0026rsquo;$이다.\n1.3. 로그 가능도 # $$ \\begin{equation} l(\\mu, \\Sigma) = C - \\frac{n}{2} \\log |\\Sigma| -\\frac{n}{2}tr\\left(\\Sigma^{-1} [S_n + (\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo;]\\right) \\end{equation} $$\n2. 빈도론 추정 # 2.1. 최대가능도 추정량 # 빈도론자의 추정량은 다음과 같이 주어진다.\n$$\\hat{\\mu}^{MLE} = \\bar{x},~\\hat{\\Sigma} = \\frac{1}{n} \\sum (x_i - \\bar{x})(x_i - \\bar{x})\u0026rsquo; = S_n$$\n$\\mu =0$임이 알려져 있으면, $\\hat{\\Sigma}^{MLE} = \\frac{1}{n} \\sum x_i x_i\u0026rsquo;$이다.\n3. 베이즈 추정 # 3.1. 베이즈 모형 # 3.1.1. 켤레 사전분포 # 다음과 같은 켤레사전분포를 생각한다.\n$$ \\begin{equation} \\begin{gathered} \\Omega \\sim W(\\nu_0, B_0^{-1}) \\\\ \\mu|\\Omega \\sim N(\\mu_0, \\Sigma/\\kappa_0) \\end{gathered} \\end{equation} $$\n여기서 $W$는 위사트(Wishart) 분포로 공분산 행렬 $\\Sigma$에 대한 사전분포를 고려한다면, 역-위샤트(inverse-Wishart) 사전분포를 고려하면 된다.\n3.1.2. 사후분포 # 사후분포는\n$\\nu_n = \\nu_0 + n,~\\kappa_n = \\kappa_0 + n,~\\mu_n = \\frac{1}{\\kappa_0 +n} (\\kappa_0 \\mu_0 + n \\bar{x}),$ $$B_n = B_0 + n S_n + \\frac{n \\kappa_0}{n+\\kappa_0} (\\mu_0 - \\bar{x}) (\\mu_0 - \\bar{x})\u0026rsquo;$$ 를 모수로 갖는 위샤트 분포로 주어진다.\n3.2. 베이즈 추정량 # 위의 사전분포로부터\n$$ \\begin{equation} \\begin{gathered} \\hat{\\mu}^B = \\mu_n \\\\ \\hat{\\Sigma}^B = \\frac{1}{\\nu_n - k - 1} B_n \\end{gathered} \\end{equation} $$\n으로 주어진다.\n3.3. 제프리스 사전분포 # 3.3.1. 사전분포 # $$ \\begin{equation}\\pi(\\mu, \\Sigma) d\\mu d\\Sigma \\propto |\\Sigma|^{-\\frac{k+1}{2}} d \\mu d\\Sigma \\end{equation}$$\n3.3.2. 사후분포 # $$\\mu|\\Sigma, \\mathbb{X} \\sim N\\left(\\bar{x},~\\frac{1}{n}\\Sigma\\right)$$ $$\\Sigma|\\mathbb{X} \\sim IW_k(k + n,~(n-1)S_n)$$\n3.3.3. 베이즈 추정량 # $$\\hat{\\mu}^B = \\bar{x}$$ $$\\hat{\\Sigma}^B = \\frac{n-1}{n-k-2} S_n$$\n3.4. 위샤트 분포 # $nu \u0026gt; k-1,~B\u0026gt;0$에 대해 양의 정부호 행렬 $W$가 위샤트 분포 $W_k(\\nu, B)$를 따른다는 것은, 다음을 의미한다.\n$$f(w)dw = \\frac{1}{2^{\\nu k / 2} |B| \\Gamma_k(\\nu/2)} |w|^\\frac{\\nu - k -1}{2} e^{-\\frac{1}{2}tr(B^{-1}w)}$$\n여기서 $dw = \\prod_{i \\leq j} dw_{ij}$를 의미한다.\n그러면, $\\mathbb{E}[W] = \\nu B$이다.\n3.5. 역-위샤트 분포 # $\\Omega \\sim IW_k (\\nu, A), ~\\nu \u0026gt; k-1,~ A \u0026gt;0$이라는 것은 다음을 의미한다.\n$$f(\\omega) d\\omega = \\frac{|A|^\\frac{\\nu - k -1}{2}}{2^\\frac{k(\\nu-k-1)}{2} \\Gamma_k(\\nu/2)} |\\omega|^{-\\frac{\\nu}{2}} e^{-\\frac{1}{2} tr(\\Omega^{-1} A)}$$\n다음이 성립한다.\n$W \\sim W_k(\\nu, B) \\Longleftrightarrow W^{-1} \\sim IW_k(\\nu+k+1, B^{-1})$. $\\Omega \\sim IW_k(\\nu, A) \\Rightarrow \\mathbb{E}[\\Omega] = \\frac{1}{\\nu - 2k - 2} A, ~ \\nu - 2k -2 \u0026gt; 0$. 3.6. $\\mu = 0$인 정규모형의 예 # 모형 $$X_1, \\cdots, X_n | \\Sigma \\stackrel{i.i.d.}{\\sim} N_k(0, \\Sigma)$$ $$\\Omega \\sim W_k(\\nu_0, B_0^{-1})$$ 의 사후분포는 $$ \\begin{equation} \\begin{gathered} \\Omega|\\mathbb{X} \\sim W_k(\\nu_0 + n, (B_0 + nS)^{-1}) \\\\ \\Sigma|\\mathbb{X} \\sim IW_k(\\nu_0 + n, B_0 + nS) \\end{gathered} \\end{equation} $$\n이 모형은 빈도론자들의 공분산 행렬 추정 모형을 그대로 옮긴 것인데, 베이즈주의자들 사이에서도 논란이 있다.\n고정된 $k$에 대해서는 사후분포, 베이즈 추정량들이 좋은 성질을 가짐이 알려져 있다.\n우리는 $k$가 변하는 경우를 함께 고려해보고자 한다.\n4. 공분산의 사용처 # 다음과 같은 분야에서 공분산 추론은 중요한 위상을 갖는다.\n주성분 분석(PCA) 판별 분석 변수들간의 독립성, 조건부 독립성 검정 정준상관분석 5. 고차원 모형 # 2000년대에 들어서, 고차원 모형에 대한 관심이 급증하였다. 고차원 모형이란, 모수의 차원 $k$가 자료의 크기 $n$과 함께 커지는 경우를 생각한다. 심지어, 다음과 같은 상황을 고려하기도 한다. $$ k \\stackrel{n \\rightarrow \\infty}{\\longrightarrow} \\infty.$$\n과거에는 자료의 크기와 관계 없이 고정된 차원을 갖는 모형들을 고려하였다.\n20세기 후반, 사람들은 \u0026lsquo;데이터 많으니 더 큰 모형을 고려할 수 있지 않을까\u0026rsquo; 하는 생각을 하기 시작했다. 즉, 자료가 커질 때, 모형의 복잡도도 함께 커지는 문제를 고려하였다. 이러한 상황에서는 기존에 알려진 모형의 점근적 성질들이 성립하지 않는 문제들이 발생하였고, 현대의 통계학은 이러한 문제를 해결하는 데 관심을 가지고 있다.\n5.1. 고차원 공분산 추정의 어려움 # $n$과 $k$가 동시에 커지면서 다음과 같은 문제가 발생한다.\n$\\dfrac{k}n$이 클수록, $\\lambda_{\\max} (S_n) \u0026raquo; \\lambda_{\\max}(\\Sigma)$이고 $\\lambda_{\\min}(S_n) \u0026laquo; \\lambda_{\\min}(\\Sigma)$이다. (Johnstone \u0026amp; Lu 2009) $S_n$의 고유벡터는 $\\Sigma$의 고유벡터로 수렴하지 않는다. 1번의 문제는 과거에도 널리 알려져 있었으며, 이를 피하기 위한 다양한 가정들이 시도되었다. 최근에는 성김(sparse)가정을 주로 한다.\n6. 공분산의 분해 # 공분산 행렬의 추정이 어려운 이유는 양의 정부호라는 제약조건 때문이다. 이를 피하기 위해 다음과 같이 공분산을 분해하여 생각하는 방법들이 제안되었다.\n6.1. 촐레스키 분해 # 촐레스키 분해(Cholesky decomposition)은 공분산 행렬을 다음과 같이 분해한다.\n$$\\Sigma = CC\u0026rsquo;,~ C = (c_{ij})$$\n여기서 $C$는 $c_{{ii}} \u0026gt; 0$인 하삼각행렬(lower triangular matrix)이다.\n증명:\n수학적 귀납법을 사용한다. $k=1$일 때는 자명하다. $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12}\u0026rsquo; \\\\ \\sigma_{12} \u0026amp; \\sigma_{22} \\end{bmatrix} = \\begin{bmatrix} C_1 \u0026amp; 0 \\\\ x\u0026rsquo; \u0026amp; y \\end{bmatrix} \\begin{bmatrix} C_1 \u0026amp; x \\\\ 0\u0026rsquo; \u0026amp; y \\end{bmatrix} $$ 을 만족하는 $x,~y$가 존재함을 보이면 된다.\n위의 식을 계산해보면, $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12}\u0026rsquo; \\\\ \\sigma_{12} \u0026amp; \\sigma_{22} \\end{bmatrix} = \\begin{bmatrix} C_1 C_1\u0026rsquo; \u0026amp; C_1 x \\\\ x\u0026rsquo; C \u0026amp; x\u0026rsquo;x + y^2 \\end{bmatrix}$$ 에서 $x = C_1^{-1} \\sigma_{12},~ y = \\sqrt{\\sigma_{22} - x\u0026rsquo;x}$이다. $\\blacksquare$\n촐레스키 분해는 간단하지만 직관적으로 통계적인 의미를 갖지 않아 잘 사용되지 않는다.\n참고: $\\Sigma$가 위샤트 분포를 따르면 $C$의 분포는 발렛 분포를 따른다는 것이 알려져 있다.\n6.2. 대각화 정리 (주성분 분석) # $\\Sigma = PDP\u0026rsquo;$와 같이 분해한다. 여기서 $P = [u_1, \\cdots, u_k]$인 직교행렬, $D= diag(\\lambda_1, \\cdots, \\lambda_k)$인 대각행렬이다.\n이 분해는 다른 문제(주성분 분석 등)에서는 유용하게 사용되나, 공분산 추정의 문제에서는 잘 사용되지 않는다.\n6.3. 수정된 촐레스키 분해 # 수정된 촐레스키 분해(modified Cholesky decomposition)는 다음과 같다.\n6.3.1. 동기 # 모형 $$ \\begin{equation} X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_k \\end{pmatrix} \\sim N(0, \\Sigma) \\end{equation} $$ 에서, 각 성분의 분포를 다음과 같이 나타낼 수 있다.\n$$ \\begin{equation} \\begin{aligned} X_1 \u0026amp;= \\epsilon_1, \\\\ X_2 \u0026amp;= a_{21} X_1 + \\epsilon_2, \\\\ X_3 \u0026amp;= a_{31} X_1 + a_{32} X_2 + \\epsilon_3, \\\\ \u0026amp;\\vdots \\\\ X_k \u0026amp;= a_{k1} X_1 + \\cdots + a_{k, k-1} X_{k-1} + \\epsilon_k \\end{aligned} \\end{equation} $$\n즉, $X = AX + \\epsilon, ~ \\epsilon \\sim N(0, D)$와 같은 형태로 나타낼 수 있다. 여기서 $$ \\begin{equation} A = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ a_{21} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{k1} \u0026amp; a_{k2} \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{equation} $$ 이고, 이러한 $A$를 촐레스키 인자라 부른다.\n그러면, $$ \\begin{equation} \\begin{gathered} (I - A) X = \\epsilon, \\\\ Var((I-A)X) = Var(\\epsilon), \\\\ (I-A) \\Sigma (I-A)\u0026rsquo; = D, \\\\ \\Sigma = (I-A)^{-1} D (I-A)\u0026rsquo;^{-1}, \\\\ \\Omega = (I-A) D^{-1} (I-A)\u0026rsquo; \\end{gathered} \\end{equation} $$\n즉, $\\Sigma$를 추정하는 공분산 추정 문제를, $A$와 $D$를 추정하는 선형회귀문제로 바꿀 수 있다.\n수정된 촐레스키 분해는 주로 사용된다.\n7. 빈도론 추정 # 7.1. 벌점함수 방법들 # $$ \\begin{equation} \\begin{aligned} \\hat{\\Sigma} \u0026amp;= \\arg\\min_{\\Sigma} \\left[ - l(\\Sigma) + \\lambda Pen(\\Sigma) \\right] \\\\ \u0026amp;= \\arg\\min_{\\Sigma} \\log |\\Sigma| + tr(\\Sigma^{-1} S_n) + \\sum_{i \u0026lt; j} P_\\lambda( \\sigma_{ij}) \\end{aligned} \\end{equation} $$\n주로 사용하는 벌점함수로는 다음이 있다.\n$P_\\lambda(\\theta) = \\lambda |\\theta|$ ($L_1$-penalty, LASSO penalty) $P_\\lambda(\\theta) = \\lambda^2 - (|\\theta| - \\lambda)^2 I(|\\theta| \u0026lt; \\lambda)$, (hard thresholding) $P_\\lambda\u0026rsquo;(\\theta) \\lambda I(|\\theta| \\leq \\lambda) + \\dfrac{(a \\lambda - \\theta)_+}{a - 1} I(|\\theta| \u0026gt; \\lambda),~ a\u0026gt;2$ 공분산 행렬의 역행렬을 계산하는 것이 비싸기 때문에, 다음과 같은 손실함수를 고려하기도 한다.\n$$ \\begin{equation} \\sum_{i, j} (s_{ij} - \\sigma_{ij})^2 + \\sum_{i \u0026lt; j} P_\\lambda( \\sigma_{ij}) \\end{equation} $$\n혹은, 다음과 같이 정밀도 행렬을 추정하는 문제를 고려하기도 한다.\n$$ \\begin{equation} \\hat{\\Omega} = \\arg\\min_{\\Omega} - \\log |\\Omega| + tr(\\Omega S_n) + \\sum_{i \u0026lt; j} P_\\lambda( \\omega_{ij}) \\end{equation} $$\n7.2. Lam \u0026amp; Fan (2009) # 빈도론의 대표적인 연구 결과를 소개한다.\nLam \u0026amp; Fan (2009)은 적당한 벌점함수에 대해 $$|\\hat{\\Sigma} - \\Sigma_0|_F^2 = O_p\\left( \\frac{(p_n + s_n) \\log p_n}{n} \\right)$$ 을 보였다. 여기서 $s_n$은 $\\Sigma$에서 0이 아닌 비대각원소의 개수, $p_n$은 차원을 의미한다.\n보통, 공분산 행렬의 추정 분제에서는 최적의 수렴속도가 다음과 같이 주어진다.\n$$\\frac{\\text{0이 아닌 모수의 개수} \\times \\log(\\text{차원})}{n}$$\n빈도론자들은 이러한 $\\hat{\\Sigma}$를 찾는 구체적인 방법들에 대해 관심을 갖는다.\n7.3. 성김 가정이 없는 추정 방법들 # 최근에는 주로 성김 가정을 하나, 이전에는 어떤 추정 방법들을 제안했나 살펴본다.\n7.3.1. Stein (1975) # $S = PDP\u0026rsquo;$와 같이 나타내자. 적당한 고유치들의 변환 $\\Lambda$에 대해 공분산 행렬의 추정량으로 $\\hat{\\Sigma} = P \\Lambda(D) P\u0026rsquo;$로 제안한다.\nJohnstone의 문제에서 알 수 있듯, 고차원 행렬 문제에서는 고유벡터를 찾는 것도 어렵기 때문에 $P$를 제대로 추정하기 어렵다.\n7.3.2. Ledoit \u0026amp; Wolf (2004) # 공분산 행렬의 추정량으로 축소 추정량 $\\hat{\\Sigma} = \\rho_1 S + \\rho_2 I$를 제안하였다.\n7.4. 성김 가정 하에서의 빈도론 추정 방법들 # Bickel \u0026amp; Levina (2008) Thresholidng, Tapering, Banding\nThresholding estimator는 다음과 같이 주어진다. $$\\begin{equation} \\begin{aligned} \\hat{\\Sigma} \u0026amp;= (\\hat{\\sigma_{ij}}) \\\\ \\hat{\\sigma}{ij} \\\\ \u0026amp;= \\begin{cases} s{ij} I\\left(|s_{ij}| \u0026gt; c \\sqrt{ \\frac{\\log p}{n}}\\right) \u0026amp; (i \\neq j) \\\\ s_{ij} \u0026amp; (i=j) \\end{cases} \\end{aligned} \\end{equation}$$\nbanding estimator는 공분산 행렬이 대각성분 근처에서만 0이 아닌 성분을 갖는 추정량을 제안한다. $$\\begin{equation} \\hat{\\Sigma} = B_k(S) = (s_{ij} I(|i-j| \\leq k)) \\end{equation}$$\ntapering estimator는 공분산 행렬이 대각성분에서 멀어질수록 0에 가까워지는 추정량을 제안한다. \\begin{equation} \\hat{\\Sigma} = T_k(S) = ( w_{ij}^{(k)} s_{ij}), \\quad w_{ij}^{(k)} = \\begin{cases} 1, \u0026amp; |i-j| \\leq \\frac{k}{2} \\\\ 2 - \\frac{|i-j|}{k/2}, \u0026amp; \\frac{k}{2} \u0026lt; |i-j| \\leq k \\\\ 0, \u0026amp; \\text{o.w.} \\end{cases} \\end{equation}\n8. 베이즈 방법 # 모수공간에 제약이 있을 때, 사전분포를 부여하는 것이 어렵다.\n8.1. 그래프 모형 # 8.1.1. 그래프 # $G = (V, E)$라 하자. $\\Omega = (\\omega_{ij})$와 같이 나타낼 때, $V = { 1,2, \\cdots, k }$, $$E \\subset V \\times V = { (i, j) : Cov(X_i, X_j) \\neq 0 \\text{ or } w_{ij} \\neq 0 }$$ 으로 정의한다.\n8.1.2. G-Wishart 분포 # $\\Omega \\sim W_G(b, D), b \u0026gt; 2, D \u0026gt; 0$는 다음을 의미한다.\n$$\\pi(\\Omega | G) = \\frac{1}{I_G(b, D)} | \\Omega|^{\\frac{b-2}{2}} e^{-\\frac{1}{2} tr(D \\Omega)} I(\\Omega \\in M_G^+)$$\n여기서 $M_G^+ = { \\Omega : \\Omega \u0026gt; 0,~ \\omega_{ij} \\neq 0 \\Leftrightarrow (i, j)\\in E }$이다.\n사후분포는 $\\Omega | \\mathbb{X}, G \\sim W_G(b+n, D+S)$로 주어진다.\n이 분포는 단순히 위샤트 분포에 제약조건을 추가한 것이라 직관적이나, 정규화 상수 $I_G(b, D)$의 계산이 사실상 불가능하다.\n이러한 문제로 분해가능(decomposible)이라는 가정을 추가한다. 분해가능하지 않을 때는 수치적으로 정규화 상수를 계산하나 차원이 커질 때 계산이 거의 불가능하다.\n8.1.3. 그래프 모형 # 이와 같은 모형을 그래프 모형(graphical model)이라 한다.\n그래프 모형에서 $w_{ij} = 0$은 $X_i \\perp X_j|X_{~(i,j)}$, 즉, 조건부 독립성을 의미한다.\n참고: $\\sigma_{ij} = 0$은 $X_i \\perp X_j$, 즉, 주변 독립성을 의미한다.\n8.1.4. G-inverse-Wishart 분포 # $\\Sigma \\sim IW_G(\\delta, U)$는 다음과 같은 밀도함수를 갖는다.\n$$\\pi(\\Sigma | G) = \\frac{1}{I_G(\\delta, U)} | \\Sigma|^{-\\frac{\\delta+2}{2}} e^{-\\frac{1}{2} tr(\\Sigma^{-1} U)} I(\\Sigma \\in M_G^+)$$\n8.1.5. 성질 # 그래프 모형은 사전분포와 사후분포가 잘 정의된다는 장점을 갖는다.\n8.2. 축소 사전분포 # 8.2.1. Wang (2015) # 공분산 행렬의 각 성분에 다음과 같은 분포를 가정하는 모형도 있다.\n$$ \\begin{equation} \\begin{aligned} \\sigma_{ij} \u0026amp;\\sim w \\delta_0 + (1-w) Normal, \\\\ \\sigma_{ii} \u0026amp; \\sim Exp \\end{aligned} \\end{equation} $$\n우리가 이번에 볼 논문은 이를 연속형 분포로 확장한 것이다.\n8.3. 사후처리 사후분포 (이광민) # post-processed posterior\n사이비 베이즈(?)\n전체 모수 공간을 $\\Theta^\\ast$, 원하는 모수 공간을 $\\Theta \\subset \\Theta^\\ast$라 하자.\n사전분포 $\\pi^\\ast$가 계산이 쉬운 사후분포 $\\pi^\\ast(\\cdot | \\mathbb{X})$를 갖는다고 하자.\n사후처리 사후분포는 다음과 같은 요소들로 구성된다.\n사후 처리 함수 $f: \\Theta^\\ast \\rightarrow \\Theta$ 사후처리 사후분포 $[f(\\theta^\\ast)|\\theta^\\ast \\sim \\pi^\\ast(\\cdot | \\mathbb{X}_n)] = \\pi(\\cdot | \\mathbf{X})$ 이 방법은 이론적 정당성을 더 확보해야 한다.\n8.4. Berger, Sun \u0026amp; Song (2020) # 다음과 같은 사전분포를 고려한다.\n$$ \\begin{equation} \\pi(\\Sigma|a, b, H) \\propto \\frac{1}{|\\Sigma|^a \\left[ \\prod_{i \u0026lt; j} (\\lambda_i - \\lambda_j) \\right]^b } e^{-\\frac{1}{2} tr(\\Sigma^{-1} H)} \\end{equation} $$\n여기서 $\\lambda_1 \u0026gt; \\lambda_2 \u0026gt; \\cdots \u0026gt; \\lambda_k$는 $\\Sigma$의 고유치이다.\n사후분포의 성질은 아직 규명되지 않았다.\n많은 곳에서는 모수의 차원으로 $p$를 사용하나 여기서는 $k$를 사용한다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":1,"href":"/2022-Q2-workshop/docs/02-sicho/","title":"공분산 행렬의 베이즈 추론","section":"2022 Q2 Bayes Workshop","content":" 모형 # $$X_1, \\cdots, X_n | \\Sigma \\sim N(0, \\Sigma)$$ $$\\Sigma = (\\sigma_{ij})_{i,j=1}^p \u0026gt; 0$$\n사전분포 # 비대각원소 # 비대각원소에는 정규혼합 사전분포를 가정한다.\n$$\\pi(\\sigma_{ij}) = (1 - \\pi) N(\\sigma_{ij}; 0, v_0^2) + \\pi N(\\sigma_{ij}; 0, v_1^2), \\quad i \\neq j$$\n대각원소 # 대각원소에는 축소 사전분포를 가정한다.\n$$\\pi(\\sigma_{ii}) = Exp(\\lambda/2)$$\n공분산 행렬 # 이를 정리하면 다음과 같다.\n$$ \\begin{equation} \\begin{aligned} \\pi(\\Sigma) \u0026amp;= \\left[ c(\\theta) \\right]^{-1} \\prod_{i \u0026lt; j} \\left[ (1 - \\pi) N(\\sigma_{ij}; 0, v_0^2) + \\pi N(\\sigma_{ij}; 0, v_1^2) \\right]\\\\ \u0026amp;\\qquad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\end{aligned} \\end{equation} $$\n여기서 $c(\\theta)$는 정규화 상수이며 모수 $\\theta = \\{v_0, v_1, \\pi, \\lambda \\}$이다.\n변수 # $Z = (z_{ij})_{i \u0026lt; j} \\in \\{0, 1 \\}^{\\frac{p(p-1)}{2}}$\n계층모형 # 이 모형은 다음과 같은 계층모형으로 나타낼 수 있다.\n\\begin{equation} \\begin{aligned} \\pi(\\Sigma|Z, \\theta) \u0026amp;= \\left[ c(\\theta) \\right]^{-1} \\prod_{i \u0026lt; j} N(\\sigma_{ij}; 0, v_{z_{ij}}^2 ) \\\\ \u0026amp;\\qquad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\\\ \\pi(Z|\\theta) \u0026amp;= \\left[ c(\\theta) \\right]^{-1}c(z, v_0, v_1, \\lambda) \\prod_{i \u0026lt; j } \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}}. \\end{aligned} \\end{equation}\nBlock-Giibs Sampler # 위의 계층모형에서 표본을 추출하는 깁스 샘플러는 다음과 같다.\n\\begin{equation} \\begin{aligned} \\pi(\\Sigma, Z|X_1, \\cdots, X_n) \u0026amp;\\propto \\prod_{i=1}^n N_p(X_i; 0, \\Sigma) \\\\ \u0026amp;\\quad \\times \\prod_{i \u0026lt; j} N(\\sigma_{ij}; 0, v_{z_{ij}}^2 ) \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}} \\\\ \u0026amp;\\quad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\\\ \u0026amp;\\propto |\\Sigma|^{-\\frac{n}{2}} \\exp\\left[ -\\frac{1}{2} tr(S\\Sigma^{-1}) \\right] \\\\ \u0026amp;\\quad \\times \\prod _ {i \u0026lt; j} \\left\\{ \\exp\\left( - \\frac{\\sigma_{ij}^2}{2v_{z_{ij}}^2} \\right) \\right \\} \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}} \\\\ \u0026amp;\\quad \\times \\prod_{i=1}^p \\exp\\left( -\\frac{\\lambda}{2} \\sigma_{ii} \\right) \\end{aligned} \\end{equation}\n$$P(z_{ij} = 1|\\Sigma, X_1, \\cdots, X_n) = \\frac{\\pi N(\\sigma_{ij}; 0, v_1^2)}{\\pi N(\\sigma_{ij} 0, v_1^2) + (1- \\pi) N(\\sigma_{ij}; 0, v_0^2)}$$\n$V = (v_{z_{ij}}^2)$은 $p \\times p$ 대칭 행렬, $v_{z_{ij}}^2 = 0$ for $i = j$.\n$$\\Sigma = \\begin{pmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12} \\\\ \\sigma_{12}^\\prime \u0026amp; \\sigma_{22} \\end{pmatrix}$$\n$$ S = X^\\prime X = \\begin{pmatrix} S_{11} \u0026amp; s_{12} \\\\ s_{12}^\\prime \u0026amp; s_{22} \\end{pmatrix}$$\n$$V = \\begin{pmatrix} V_{11} \u0026amp; v_{12} \\\\ v_{12}^\\prime \u0026amp; 0 \\end{pmatrix}$$\n이제 다음과 같은 변환을 생각하자. $$(\\sigma_{12},~\\sigma_{22}) \\mapsto (u=\\sigma_{12},~v=\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12})$$\n이 변환의 야코비안은 다음과 같이 계산된다.\n\\begin{equation} |J| = \\left|\\begin{pmatrix} 1 \u0026amp; 0 \\\\ -2\\Sigma_{11}^{-1} \\sigma_{12} \u0026amp; 1 \\end{pmatrix}\\right| = 1 \\end{equation}\n그러면, 공분산의 역행렬은 \\begin{equation} \\begin{aligned} \\Sigma^{-1} \u0026amp;= \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} \\sigma_{12} \\left( \\sigma_{22} - \\sigma_{21}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}\\right) \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \u0026amp; \u0026hellip; \\\\ -(\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1}\\sigma_{12})^{-1}\\sigma_{12}^\\prime \\Sigma_{11}^{-1} \u0026amp; \\left(\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}\\right)^{-1} \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} u u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; -\\Sigma_{11}^{-1} uv^{-1} \\\\ -u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; v^{-1} \\end{pmatrix} \\end{aligned} \\end{equation}\n따라서, \\begin{equation} \\begin{aligned} |\\Sigma| \u0026amp;= |\\Sigma_{11}| |\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12} | \\\\ \u0026amp;= |\\Sigma_{11}| (\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1}\\sigma_{12}) \\\\ \u0026amp;\\propto v, \\\\ tr(S\\Sigma^{-1}) \u0026amp;= tr\\left[ \\begin{pmatrix} S_{11} \u0026amp; s_{12} \\\\ s_{21}^\\prime \u0026amp; s_{22} \\end{pmatrix} \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} u u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; -\\Sigma_{11}^{-1} uv^{-1} \\\\ -u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; v^{-1} \\end{pmatrix} \\right] \\\\ \u0026amp;\\propto u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u v^{-1} -2 s_{12}^\\prime \\Sigma_{11}^{-1} u v^{-1} + s_22 v^{-1} \\end{aligned} \\end{equation}\n또한, \\begin{equation} \\prod_{i \u0026lt; j} \\exp\\left( - \\frac{\\sigma_j^2}{2 v_{z_{ij}}^2} \\right) \\propto \\exp\\left( - \\frac{1}{2} u^\\prime D^{-1} v \\right), \\end{equation} 여기서 $D = diag(v_{12}),~ v = \\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}$이다. \\begin{equation} \\prod_{i=1}^p \\exp\\left( - \\frac{\\lambda}{2} \\sigma_{ii} \\right) \\propto \\exp\\left( - \\frac{\\lambda}{2} \\left( u^\\prime \\Sigma_{11}^{-1} u + v \\right) \\right) \\end{equation} 에서,\n\\begin{equation} \\log \\pi(u, v | \\cdot ) \\propto -\\frac{1}{2} \\left \\{ n \\log v + u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u v^{-1} - 2 s_{12}^\\prime \\Sigma_{11}^{-1} u v^{-1} + s_{22} v^{-1} + u^\\prime D^{-1} u + \\lambda u^\\prime \\Sigma_{11}^{-1} u + \\lambda v \\right \\}, \\end{equation}\n\\begin{equation} \\begin{gathered} \\pi(u|v, z, \u0026hellip;) = N \\left( (B+D^{-1})^{-1} w, (B+D^{-1})^{-1} \\right), \\\\ B = \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} v^{-1} + \\lambda \\Sigma_{11}^{-1}, \\\\ w = \\Sigma_{11}^{-1} s_{12} v^{-1}. \\end{gathered} \\end{equation}\n이는 Generalized inverse Gaussian, $GIG(q, a, b)$로, 그 확률밀도함수는 \\begin{equation} f(x) = \\frac{(a/b)^{q/2}}{2K_q (\\sqrt{ab})} \\lambda^{p-1}e^{-(ax + b/x)/2} \\end{equation} 로 주어진다.\n즉, $\\pi(v|u, z, \u0026hellip;) = GIG\\left(1 - n/2, \\lambda, u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u - 2 s_{12}^\\prime \\Sigma_{11}^{-1} u + s_{22}\\right)$이다.\n이 분포들에서 순서대로 표본을 추출하면 된다.\n참고: 실제로 이를 구현하면 수치적 오류로 인해 알고리듬이 잘 돌아가지 않는다.\n논문의 사전분포 # Armigan?\n$\\Sigma = (\\sigma_{ij})$ $\\rho = (\\rho_{ij})$ $\\pi(\\Sigma, \\rho) = \\prod_{i \u0026gt; j} N\\left( \\sigma_{ij}; 0, \\frac{\\rho_{ij}}{1-\\rho_{ij}} \\tau^2 \\right) Beta(\\rho_{ij}; a, b) \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2} \\right)$ $v = (v_{ij}^2) = \\begin{pmatrix} V_{11} \u0026amp; v_{12} \\\\ v_{12}^\\prime \u0026amp; 0 \\end{pmatrix},~ v_{ij}^2 = \\dfrac{\\rho_{ij}}{1 - \\rho_{ij}} \\tau^2$ $\\phi_{ij} = \\dfrac{\\rho_{ij}}{1 - \\rho_{ij}}$라 하자. 그러면, \\begin{equation} \\begin{aligned} \\sigma_{ij}|\\phi_{ij} \u0026amp;\\sim N(0, \\phi_{ij} \\tau^2), \\\\ \\phi_{ij}|\\psi_{ij} \u0026amp;\\sim \\text{Gamma}(a, \\psi_{ij}), \\\\ \\psi_{ij} \u0026amp;\\sim \\text{Gamma}(b, 1) \\end{aligned} \\end{equation} 에서 \\begin{equation} \\begin{aligned} \\pi(\\psi_{ij}|\\phi_{ij}, \u0026hellip;) \u0026amp;\\propto \\psi_{ij}^{b-1}e^{-\\psi_{ij}} \\phi_{ij}^{a-1} e^{-\\psi_{ij} \\phi_{ij}} \\psi_{ij}^a \\\\ \u0026amp;= \\psi_{ij}^{a+b-1} e^{- (\\phi_{ij} + 1)\\psi_{ij}} \\\\ \u0026amp;= \\text{Gamma}(\\cdot, a+b, \\phi_{ij}+ 1). \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} \\pi(\\phi_{ij}|\u0026hellip;) \u0026amp;\\propto \\phi_{ij}^{-\\frac{1}{2}}e^{-\\frac{\\sigma_{ij}^2}{2\\phi_{ij} \\tau^2}} \\phi_{ij}^{a-1} e^{-\\psi_{ij} \\phi_{ij}} \\\\ \u0026amp;= \\phi_{ij}^{a-\\frac{1}{2}-1}e^{-\\frac{\\sigma_{ij}^2}{2\\phi_{ij} \\tau^2} - \\psi_{ij} \\phi_{ij}} \\\\ \u0026amp;= GIG\\left(a - \\frac{1}{2},~ 2\\psi_{ij},~ \\frac{\\sigma_{ij}^2}{\\tau^2}\\right) \\end{aligned} \\end{equation}\n논문의 의의 # 축소 사전분포의 사용 이론적 성질의 규명 "},{"id":2,"href":"/2022-Q2-workshop/docs/03/","title":"Theorem 3 and Lemma 2","section":"2022 Q2 Bayes Workshop","content":"TBA\n"},{"id":3,"href":"/2022-Q2-workshop/docs/04/","title":"Theorem 4 and Lemma 3","section":"2022 Q2 Bayes Workshop","content":"TBA\n"},{"id":4,"href":"/2022-Q2-workshop/docs/05/","title":"Lemma 4 and Theorem 5","section":"2022 Q2 Bayes Workshop","content":" (Lower bound for $\\pi(B_n)$) 만일 다음의 조건들이 만족되면 $\\Sigma_0 \\in \\mathcal{U}(s_0, \\zeta_0)$ with $\\zeta_0 \u0026lt; \\zeta$ $p \\asymp n^\\beta$ with $0 \u0026lt; \\beta\u0026lt; 1$ $\\zeta^\\ast \\leq p$ $\\zeta^2 \\zeta_0^2 \\leq s_0 \\log p$ $n \\geq \\max\\{ 1/ \\zeta_0^\\ast,~ s_0 / (1 - \\zeta_0 / \\zeta)^2 \\} \\log p / \\zeta^\\ast$ $p^{-1} \u0026lt; \\lambda \u0026lt; \\log p / \\zeta_0$ $a=b=1/2$ $(p^2 \\sqrt{n})^{-1} \\lesssim \\tau \\lesssim (p^2 \\sqrt{n})^{-1} \\sqrt{s_0 \\log p}$ (From page 5, Theorem 1) $(p + s_0) \\log p = o(n)$ (From page 5) $p = O(s_n)$ 다음이 성립한다\n\\begin{equation} \\pi(B _ {\\epsilon _ n}) \\geq \\exp \\left\\{ - \\left( s + \\frac{1}{\\beta} \\right) n \\epsilon _ n^2 \\right \\}. \\end{equation}\n여기서 $\\epsilon_n = \\sqrt{\\frac{(p+s_0) \\log p}{n}},$ $$B_{\\epsilon} = \\{ f_{\\Sigma} : \\Sigma \\in \\mathcal{C} _ p, ~ K(f_{\\Sigma_0}, f_{\\Sigma}) \u0026lt; \\epsilon^2,~ V(f_{\\Sigma_0}, f_{\\Sigma}) \u0026lt; \\epsilon^2 \\}$$ 이다.\n이 정리의 증명을 위해서 Lemma 3와 Lemma 4가 필요하다.\n"},{"id":5,"href":"/2022-Q2-workshop/docs/06/","title":"Theorem 2 ","section":"2022 Q2 Bayes Workshop","content":" 모형 # 다음의 모형을 생각하자.\n$$ \\begin{equation}\\label{eqn-model} X_1, \\cdots, X_n | \\Sigma \\sim N(0, \\Sigma) \\end{equation} $$\n양의 정수 $s_0$와 실수 $\\zeta_0 \u0026gt; 1$에 대해 다음과 같은 모수공간을 생각한다. \\begin{equation} U(s_0, \\zeta_0) = \\{ \\Sigma \\in C_p: s(\\Sigma) \\leq s_0,~ \\zeta_0^{-1} \\leq \\lambda_{\\min}(\\Sigma) \\leq \\lambda_{\\max}(\\Sigma) \\leq \\zeta_0 \\} \\end{equation} 여기서 $s(\\Sigma)$는 행렬 $\\Sigma$의 0이 아닌 비대각성분의 개수를 의미한다.\n정리 # 모형 (1)과 양의 정수 $s_0$ 와 실수 $\\zeta_0 \u003e 1$에 대해 $\\Sigma_0 \\in \\mathcal{U}(s_0, \\zeta_0)$이라 하자. $s_0^2 (\\log p)^3 = O(p^2n)$이면 작은 상수 $\\epsilon \u003e 0$에 대해 다음이 성립한다. \\begin{equation} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in \\mathcal{U}(s_0, \\zeta_0)} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{(p+s_0) \\log p}{n} I\\left(3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}\\right) + \\frac{p+s_0}{n} \\end{equation}\n이 정리는 공분산 추정의 minimax lower bound를 알려준다.\n논문의 Theorem 1에서는 사후수렴속도가 $\\dfrac{(p+s_0) \\log p}{n}$임을 보였는데, 이는 $3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}$일 때 베이즈 추론이 minimax 이고, 그렇지 않은 경우에도 nearly minimax $(\\log p)$ 임을 의미한다.\n이와 관련된 연구로 Cai와 Zhou (2012)1가 있는데, 이 논문에서는 빈도론 관점에서 성긴 공분산 행렬을 추론하는 문제를 다루었다. 다만, 논문에서는 공분산 행렬의 각 열의 0이 아닌 성분에 대한 제약조건을 다루었으나, 본 논문에서는 전체 행렬에서 0이 아닌 성분에 대한 제약조건에 대해 다룬다.\n증명 # 다음의 두 항목을 증명하면 된다. $3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}$인 경우, $$\\begin{equation}\\label{eqn-13} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in B_1} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{s_0 \\log p}{n} \\end{equation}$$ 이 성립하는 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$가 존재함을 보인다. (이경원, 정진욱) 나머지 경우, $$\\begin{equation}\\label{eqn-14} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in B_2} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{s_0 + p}{n} \\end{equation}$$ 이 성립하는 $B_2 \\subset \\mathcal{U}(s_0, \\zeta_0)$가 존재함을 보인다. (김성민) 먼저 첫 항목을 보이자. $\\nu = \\sqrt{\\epsilon/4}$에 대해 $r = \\lfloor p/2 \\rfloor,~\\epsilon_{np} = \\nu \\sqrt{\\log p / n}$이라 하자. $A_m(u)$를 $m$번째 행과 열이 $u$의 값을 갖고, 나머지에서 모두 0인 대칭행렬이라 하자. 즉, \\begin{equation} (A_m(u))_{ij} = \\begin{cases} u \u0026amp; i = m \\text{ or} j = m \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\end{equation}\n이라 하자. 이제, 다음과 같이 $B_1$을 정의한다.\n\\begin{equation} B_1 := \\left\\{ \\Sigma(\\theta) : \\Sigma(\\theta) = I_p + \\epsilon_{np} \\sum_{m=1}^r \\gamma_m A_m(\\lambda_m),~\\theta = (\\gamma, \\lambda) \\in \\Theta \\right\\} \\end{equation}\n여기서 $\\gamma = (\\gamma_1,\\cdots, \\gamma_r) \\in \\Gamma = [0, 1]^r$, $\\lambda = (\\lambda_1, \\cdots, \\lambda_r)^T \\in \\Lambda \\subset \\mathbb{R}^{r \\times p}$, \\begin{equation} \\begin{aligned} \\Lambda= \\bigg\\{ \\lambda = (\\lambda_{ij}) : \u0026amp;\\lambda_{mi} \\in \\{0, 1\\},~ \\lVert \\lambda_m \\rVert_0 = k,~\\sum_{i=1}^{p-r} \\lambda_{mi} = 0, \\\\ \u0026amp;m \\in \\{ 1, \\cdots, r \\},~ \\text{ satisfying } \\max_{1 \\leq i \\leq p} \\sum_{m=1}^r \\lambda_{mi} \\leq 2k \\bigg\\}, \\end{aligned} \\end{equation} $k = \\lceil c_{np} / 2 \\rceil - 1,~ c_{np} = \\lceil s_0 / p \\rceil$, $\\Theta = \\Gamma \\times \\Lambda$이다.\n이제 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$과 논문의 식 (13)이 성립함을 보이면 된다.\n먼저, 임의의 $\\zeta_0 \u0026gt;1$과 충분히 큰 $n$에 대해 $\\Sigma(\\theta) \\in B_1$의 가장 큰 고유치가 $\\zeta_0$보다 작다는 것은 다음과 같이 보일 수 있다. \\begin{equation} \\lambda_{\\max}\\left(\\Sigma(\\theta)\\right) \\leq \\lVert \\Sigma(\\theta) \\rVert_1 \\leq 1 + 2 k \\epsilon_{np} \\leq 1 + c_{np} \\nu \\sqrt{\\log p / n} \\leq \\zeta_0 \\end{equation} 두 번째 부등호는 모수공간 $\\Lambda$의 마지막 조건으로부터, 마지막 부등호는 가정 $s_0^2 (\\log p)^3 = O(p^2 n)$에 의해 성립한다.\n마찬가지의 이유로 임의의 $\\zeta_0 \u0026gt;1$과 충분히 큰 $n$에 대해 \\begin{equation} 2k\\epsilon_{np} \\leq c_{np} \\nu \\sqrt{\\log p / n}\\leq \\left( 1 + \\frac{s_0}{p} \\right) \\nu \\sqrt{\\log p / n} \\leq 1 - \\zeta_0^{-1} \\end{equation} 가 성립하므로 $\\Sigma(\\theta) - \\zeta_0^{-1} I_p$는 대각지배(diagonally dominant)행렬이고, 대칭이며 모든 성분이 0보다 크거나 같아 양의 준정부호 행렬이다2. 따라서, $\\Sigma(\\theta)$의 가장 작은 고유치는 $\\zeta_0^{-1}$보다 크다.\n마지막으로 $\\Sigma(\\theta)$의 비대각성분은 모두 $A_m$들에 의해서만 나타나므로 \\begin{equation} s(\\Sigma(\\theta)) \\leq 2 kp \\leq s_0 \\end{equation} 에서 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$를 얻는다.\n이제 논문의 식 (13)이 성립함을 보이자. 이를 위해, 다음의 보조정리를 소개한다. 이 보조정리는 모수공간 $\\Theta$에서 거리 $d$를 갖는 거리공간으로의 변환 $\\psi(\\theta)$의 최대 위험의 하한을 알려준다.\n(Lemma 3 of Cai and Zhou (2012)) OPTIMAL RATES OF CONVERGENCE FOR SPARSE COVARIANCE MATRIX ESTIMATION\nFor any $s \u0026gt; 0$ and any estimator $T$ of $\\psi(\\theta)$ based on an observation from the experiment $\\{ P_\\theta,~\\theta \\in \\Theta \\}$,\n\\begin{equation} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \\geq \\alpha \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert, \\end{equation}\nwhere $\\overline{\\mathbb{P}} _ {i, a}$ is the mixture distribution over all $P_\\theta$ with $\\gamma_i(\\theta)$ ﬁxed to be a while all other components of $\\theta$ vary over all possible values, i.e., \\begin{equation} \\overline{\\mathbb{P}} _ {i, a} = \\frac{1}{2^{r-1} |\\Lambda|} \\sum_{\\theta \\in \\Theta_{i, a}} P_\\theta, \\end{equation}\nfor $\\Theta_{i, a} = \\{ \\theta \\in \\Theta: \\gamma_i(\\theta) = a \\}$,\n\\begin{equation} \\lVert \\mathbb{P} \\wedge \\mathbb{Q} \\rVert = \\int (p \\wedge q) d \\mu, \\end{equation} for probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ which have densities $p$ and $q$ respectively, $\\mathbb{E} _ \\theta$ is expectation with respect to $[X_1, \\cdots, X_n | \\theta]$, $H(x, y)$ is the Hamming distance defined as $$H(x, y) = \\sum_{j=1}^r |x_i - y_i|, \\quad x, y \\in \\{ 0, 1 \\}^r.$$\n\\begin{equation} \\alpha = \\min_{(\\theta, \\theta^\\prime) : H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\geq 1} d^s(\\psi(\\theta), \\psi(\\theta^\\prime)) / H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{equation}\n최댓값은 평균보다 크거나 같으므로 \\begin{equation} \\begin{aligned} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} 2^s \\mathbb{E} _ \\theta d^s(T, \\psi(\\theta)) \\\\ \u0026amp;= \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta (2 d(T, \\psi(\\theta)))^s \\end{aligned} \\end{equation}\n$\\hat{\\theta} := \\arg\\min d^s(T, \\psi(\\theta))$라 하면 (유일하지 않다면 적당히 하나를 잡으면 된다.)\n\\begin{equation} \\begin{aligned} \\mathbb{E} _ \\theta (2 d(T, \\psi(\\theta)))^s \u0026amp;\\geq \\mathbb{E} _ \\theta (d(T, \\psi(\\theta)) + d(T, \\psi(\\hat{\\theta})) )^s \\\\ \u0026amp;\\geq \\mathbb{E} _ \\theta (d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s \\end{aligned} \\end{equation} 를 얻는다. 마지막 부등식에서 삼각부등식을 사용하였다.\n정리하면 다음을 얻는다.\n\\begin{equation} \\begin{aligned} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta (d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s \\\\ \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[\\frac{(d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s}{H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\vee 1 } H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\\\ \u0026amp;\\geq \\alpha \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\end{aligned} \\end{equation}\n이제 \\begin{equation} \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\geq \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{equation} 을 보이면 원하는 결과를 얻는다.\n\\begin{equation} \\begin{aligned} \u0026amp;\\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\\\ \u0026amp;= \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\sum_{i=1}^r \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\\\ \u0026amp;= \\sum_{i=1}^r \\frac{1}{2^r |\\Lambda| } \\sum_{\\rho \\in \\Gamma} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta^\\prime))| \\right] + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |1 - \\gamma_i(\\theta^\\prime))| \\right] \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) d\\mathbb{P}_ {\\theta^\\prime} + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\int _ {\\theta^\\prime} 1 - \\gamma_i(\\theta^\\prime)) d\\mathbb{P}_ {\\theta^\\prime} \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} d\\mathbb{P}_ {\\theta^\\prime} + \\int _ {\\theta^\\prime} \\left( 1 - \\gamma_i(\\theta^\\prime)) \\right) \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} d\\mathbb{P}_ {\\theta^\\prime} \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) d \\overline{\\mathbb{P}} _ {i, 0} + \\int _ {\\theta^\\prime} \\left( 1 - \\gamma_i(\\theta^\\prime)) \\right) d \\overline{\\mathbb{P}} _ {i, 0} \\right\\} \\\\ \u0026amp;\\geq \\frac{1}{2} \\sum_{i=1}^r \\int d \\left[ \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\right] \\\\ \u0026amp;\\geq \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{aligned} \\end{equation}\n증명이 끝났다.\n위의 보조정리에 $s=2$를 대입하면 다음의 부등식을 얻는다. \\begin{equation} \\inf_{\\hat{\\Sigma}} \\max_{\\theta \\in \\Theta} 2^2 \\mathbb{E}_\\theta \\lVert \\hat{\\Sigma}- \\Sigma(\\theta) \\rVert_F^2 \\geq \\alpha \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{equation}\n여기서 \\begin{equation} \\alpha = \\min_{(\\theta, \\theta^\\prime) : H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\geq 1} \\lVert \\Sigma(\\theta) - \\Sigma(\\theta^\\prime) \\rVert_F^2 / H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{equation} 이다.\n이때 임의의 $\\theta,~\\theta^\\prime \\in \\Theta$에 대해 \\begin{equation} \\begin{aligned} \\lVert \\Sigma(\\theta) - \\Sigma(\\theta^\\prime) \\rVert_F^2 \u0026amp;= \\epsilon_{np}^2 \\left\\lVert \\sum_{m=1}^r \\gamma_m(\\theta) A_m(\\lambda_m(\\theta)) - \\sum_{m=1}^r \\gamma_m(\\theta^\\prime) A_m(\\lambda_m(\\theta^\\prime)) \\right\\rVert_F^2 \\\\ \u0026amp; \\geq 2k\\epsilon_{np}^2 H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{aligned} \\end{equation} 이므로 $k$와 $r$의 정의($r = \\lfloor p/2 \\rfloor,~k = \\lceil c_{np} / 2 \\rceil - 1,~ c_{np} = \\lceil s_0 / p \\rceil$)로부터 다음을 얻는다.\n\\begin{equation} \\alpha r \\geq 2k\\epsilon_{np}^2 r \\geq \\nu^2 \\left( \\frac{1}{2} - \\frac{p}{s_0} \\right) \\frac{s_0 \\log p}{n} \\asymp \\frac{s_0 \\log p}{n} \\end{equation}\n이제, 다음을 만족하는 적당한 상수 $c_1 \u0026gt; 0$이 존재함을 보이면 증명이 끝난다. \\begin{equation} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\geq c_1 \\end{equation}\nTBA\nT.T. Cai, H.H. Zhou, Optimal rates of convergence for sparse covariance matrix estimation, Ann. Statist. 40 (5) (2012) 2389–2420.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Hermitian diagonally dominant matrix $A$ with real non-negative diagonal entries is positive semidefinite. From https://en.wikipedia.org/wiki/Diagonally_dominant_matrix#Applications_and_properties 혹은, 더 간단하게 Gershgorin circle theorem에 symmetric matrix가 real eigenvalue를 가진다는 사실로도 보일 수 있다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]