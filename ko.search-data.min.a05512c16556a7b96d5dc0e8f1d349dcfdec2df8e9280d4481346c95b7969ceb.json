[{"id":0,"href":"/2022-Q2-workshop/docs/01-overview/","title":"공분산 행렬 추정 문제","section":"2022 Q2 Bayes Workshop","content":" 1. 모형 # 1.1. 다변량 정규 모형 # $\\mu \\in \\mathbb{R}^k,~\\Sigma \\in \\mathbb{R}^{k \\times k},~\\Sigma \u0026gt; 0$에 대해 다음과 같은 모형을 생각하자1.\n$$ \\begin{equation} X_1, \\cdots, X_n | \\mu, \\Sigma \\stackrel{i.i.d.}{\\sim} N(\\mu, \\Sigma) \\end{equation} $$\n정밀도 행렬은 $\\Omega = \\Sigma^{-1}$로 정의된다.\n여기서 $\\mu$와 $\\Sigma$를 추정하는 것이 문제이다.\n일반적으로는, 평균의 추정보다 공분산 추정이 어려운데, 이는 \u0026lsquo;양의 정부호 행렬\u0026rsquo;이라는 제약이 있기 때문이다.\n1.2. 가능도 # $$ \\begin{equation} \\begin{aligned} L(\\mu, \\Sigma) \u0026amp;= \\prod_{i=1}^n N(x_i, \\mu, \\Sigma) \\\\ \u0026amp;= \\prod_{i=1}^n |2\\pi \\Sigma|^{-1/2} e^{-\\frac{1}{2}(x_i -\\mu)\u0026rsquo; \\Sigma^{-1} (x_i -\\mu)} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{1}{2}(x_i -\\mu)\u0026rsquo; \\Sigma^{-1} (x_i -\\mu)} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{1}{2} tr(\\Sigma^{-1} (x_i -\\mu) (x_i -\\mu)\u0026rsquo;} \\\\ \u0026amp;\\propto | \\Sigma|^{-n/2} \\prod_{i=1}^n e^{-\\frac{n}{2} tr(\\Sigma^{-1} [S_n + (\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo;])} \\end{aligned} \\end{equation} $$\n여기서 $nS_n + n(\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo; = (x_i -\\mu) (x_i -\\mu)\u0026rsquo;$이다.\n1.3. 로그 가능도 # $$ \\begin{equation} l(\\mu, \\Sigma) = C - \\frac{n}{2} \\log |\\Sigma| -\\frac{n}{2}tr\\left(\\Sigma^{-1} [S_n + (\\bar{x} - \\mu)(\\bar{x} - \\mu)\u0026rsquo;]\\right) \\end{equation} $$\n2. 빈도론 추정 # 2.1. 최대가능도 추정량 # 빈도론자의 추정량은 다음과 같이 주어진다.\n$$\\hat{\\mu}^{MLE} = \\bar{x},~\\hat{\\Sigma} = \\frac{1}{n} \\sum (x_i - \\bar{x})(x_i - \\bar{x})\u0026rsquo; = S_n$$\n$\\mu =0$임이 알려져 있으면, $\\hat{\\Sigma}^{MLE} = \\frac{1}{n} \\sum x_i x_i\u0026rsquo;$이다.\n3. 베이즈 추정 # 3.1. 베이즈 모형 # 3.1.1. 켤레 사전분포 # 다음과 같은 켤레사전분포를 생각한다.\n$$ \\begin{equation} \\begin{gathered} \\Omega \\sim W(\\nu_0, B_0^{-1}) \\\\ \\mu|\\Omega \\sim N(\\mu_0, \\Sigma/\\kappa_0) \\end{gathered} \\end{equation} $$\n여기서 $W$는 위사트(Wishart) 분포로 공분산 행렬 $\\Sigma$에 대한 사전분포를 고려한다면, 역-위샤트(inverse-Wishart) 사전분포를 고려하면 된다.\n3.1.2. 사후분포 # 사후분포는\n$\\nu_n = \\nu_0 + n,~\\kappa_n = \\kappa_0 + n,~\\mu_n = \\frac{1}{\\kappa_0 +n} (\\kappa_0 \\mu_0 + n \\bar{x}),$ $$B_n = B_0 + n S_n + \\frac{n \\kappa_0}{n+\\kappa_0} (\\mu_0 - \\bar{x}) (\\mu_0 - \\bar{x})\u0026rsquo;$$ 를 모수로 갖는 위샤트 분포로 주어진다.\n3.2. 베이즈 추정량 # 위의 사전분포로부터\n$$ \\begin{equation} \\begin{gathered} \\hat{\\mu}^B = \\mu_n \\\\ \\hat{\\Sigma}^B = \\frac{1}{\\nu_n - k - 1} B_n \\end{gathered} \\end{equation} $$\n으로 주어진다.\n3.3. 제프리스 사전분포 # 3.3.1. 사전분포 # $$ \\begin{equation}\\pi(\\mu, \\Sigma) d\\mu d\\Sigma \\propto |\\Sigma|^{-\\frac{k+1}{2}} d \\mu d\\Sigma \\end{equation}$$\n3.3.2. 사후분포 # $$\\mu|\\Sigma, \\mathbb{X} \\sim N\\left(\\bar{x},~\\frac{1}{n}\\Sigma\\right)$$ $$\\Sigma|\\mathbb{X} \\sim IW_k(k + n,~(n-1)S_n)$$\n3.3.3. 베이즈 추정량 # $$\\hat{\\mu}^B = \\bar{x}$$ $$\\hat{\\Sigma}^B = \\frac{n-1}{n-k-2} S_n$$\n3.4. 위샤트 분포 # $nu \u0026gt; k-1,~B\u0026gt;0$에 대해 양의 정부호 행렬 $W$가 위샤트 분포 $W_k(\\nu, B)$를 따른다는 것은, 다음을 의미한다.\n$$f(w)dw = \\frac{1}{2^{\\nu k / 2} |B| \\Gamma_k(\\nu/2)} |w|^\\frac{\\nu - k -1}{2} e^{-\\frac{1}{2}tr(B^{-1}w)}$$\n여기서 $dw = \\prod_{i \\leq j} dw_{ij}$를 의미한다.\n그러면, $\\mathbb{E}[W] = \\nu B$이다.\n3.5. 역-위샤트 분포 # $\\Omega \\sim IW_k (\\nu, A), ~\\nu \u0026gt; k-1,~ A \u0026gt;0$이라는 것은 다음을 의미한다.\n$$f(\\omega) d\\omega = \\frac{|A|^\\frac{\\nu - k -1}{2}}{2^\\frac{k(\\nu-k-1)}{2} \\Gamma_k(\\nu/2)} |\\omega|^{-\\frac{\\nu}{2}} e^{-\\frac{1}{2} tr(\\Omega^{-1} A)}$$\n다음이 성립한다.\n$W \\sim W_k(\\nu, B) \\Longleftrightarrow W^{-1} \\sim IW_k(\\nu+k+1, B^{-1})$. $\\Omega \\sim IW_k(\\nu, A) \\Rightarrow \\mathbb{E}[\\Omega] = \\frac{1}{\\nu - 2k - 2} A, ~ \\nu - 2k -2 \u0026gt; 0$. 3.6. $\\mu = 0$인 정규모형의 예 # 모형 $$X_1, \\cdots, X_n | \\Sigma \\stackrel{i.i.d.}{\\sim} N_k(0, \\Sigma)$$ $$\\Omega \\sim W_k(\\nu_0, B_0^{-1})$$ 의 사후분포는 $$ \\begin{equation} \\begin{gathered} \\Omega|\\mathbb{X} \\sim W_k(\\nu_0 + n, (B_0 + nS)^{-1}) \\\\ \\Sigma|\\mathbb{X} \\sim IW_k(\\nu_0 + n, B_0 + nS) \\end{gathered} \\end{equation} $$\n이 모형은 빈도론자들의 공분산 행렬 추정 모형을 그대로 옮긴 것인데, 베이즈주의자들 사이에서도 논란이 있다.\n고정된 $k$에 대해서는 사후분포, 베이즈 추정량들이 좋은 성질을 가짐이 알려져 있다.\n우리는 $k$가 변하는 경우를 함께 고려해보고자 한다.\n4. 공분산의 사용처 # 다음과 같은 분야에서 공분산 추론은 중요한 위상을 갖는다.\n주성분 분석(PCA) 판별 분석 변수들간의 독립성, 조건부 독립성 검정 정준상관분석 5. 고차원 모형 # 2000년대에 들어서, 고차원 모형에 대한 관심이 급증하였다. 고차원 모형이란, 모수의 차원 $k$가 자료의 크기 $n$과 함께 커지는 경우를 생각한다. 심지어, 다음과 같은 상황을 고려하기도 한다. $$ k \\stackrel{n \\rightarrow \\infty}{\\longrightarrow} \\infty.$$\n과거에는 자료의 크기와 관계 없이 고정된 차원을 갖는 모형들을 고려하였다.\n20세기 후반, 사람들은 \u0026lsquo;데이터 많으니 더 큰 모형을 고려할 수 있지 않을까\u0026rsquo; 하는 생각을 하기 시작했다. 즉, 자료가 커질 때, 모형의 복잡도도 함께 커지는 문제를 고려하였다. 이러한 상황에서는 기존에 알려진 모형의 점근적 성질들이 성립하지 않는 문제들이 발생하였고, 현대의 통계학은 이러한 문제를 해결하는 데 관심을 가지고 있다.\n5.1. 고차원 공분산 추정의 어려움 # $n$과 $k$가 동시에 커지면서 다음과 같은 문제가 발생한다.\n$\\dfrac{k}n$이 클수록, $\\lambda_{\\max} (S_n) \u0026raquo; \\lambda_{\\max}(\\Sigma)$이고 $\\lambda_{\\min}(S_n) \u0026laquo; \\lambda_{\\min}(\\Sigma)$이다. (Johnstone \u0026amp; Lu 2009) $S_n$의 고유벡터는 $\\Sigma$의 고유벡터로 수렴하지 않는다. 1번의 문제는 과거에도 널리 알려져 있었으며, 이를 피하기 위한 다양한 가정들이 시도되었다. 최근에는 성김(sparse)가정을 주로 한다.\n6. 공분산의 분해 # 공분산 행렬의 추정이 어려운 이유는 양의 정부호라는 제약조건 때문이다. 이를 피하기 위해 다음과 같이 공분산을 분해하여 생각하는 방법들이 제안되었다.\n6.1. 촐레스키 분해 # 촐레스키 분해(Cholesky decomposition)은 공분산 행렬을 다음과 같이 분해한다.\n$$\\Sigma = CC\u0026rsquo;,~ C = (c_{ij})$$\n여기서 $C$는 $c_{{ii}} \u0026gt; 0$인 하삼각행렬(lower triangular matrix)이다.\n증명:\n수학적 귀납법을 사용한다. $k=1$일 때는 자명하다. $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12}\u0026rsquo; \\\\ \\sigma_{12} \u0026amp; \\sigma_{22} \\end{bmatrix} = \\begin{bmatrix} C_1 \u0026amp; 0 \\\\ x\u0026rsquo; \u0026amp; y \\end{bmatrix} \\begin{bmatrix} C_1 \u0026amp; x \\\\ 0\u0026rsquo; \u0026amp; y \\end{bmatrix} $$ 을 만족하는 $x,~y$가 존재함을 보이면 된다.\n위의 식을 계산해보면, $$\\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12}\u0026rsquo; \\\\ \\sigma_{12} \u0026amp; \\sigma_{22} \\end{bmatrix} = \\begin{bmatrix} C_1 C_1\u0026rsquo; \u0026amp; C_1 x \\\\ x\u0026rsquo; C \u0026amp; x\u0026rsquo;x + y^2 \\end{bmatrix}$$ 에서 $x = C_1^{-1} \\sigma_{12},~ y = \\sqrt{\\sigma_{22} - x\u0026rsquo;x}$이다. $\\blacksquare$\n촐레스키 분해는 간단하지만 직관적으로 통계적인 의미를 갖지 않아 잘 사용되지 않는다.\n참고: $\\Sigma$가 위샤트 분포를 따르면 $C$의 분포는 발렛 분포를 따른다는 것이 알려져 있다.\n6.2. 대각화 정리 (주성분 분석) # $\\Sigma = PDP\u0026rsquo;$와 같이 분해한다. 여기서 $P = [u_1, \\cdots, u_k]$인 직교행렬, $D= diag(\\lambda_1, \\cdots, \\lambda_k)$인 대각행렬이다.\n이 분해는 다른 문제(주성분 분석 등)에서는 유용하게 사용되나, 공분산 추정의 문제에서는 잘 사용되지 않는다.\n6.3. 수정된 촐레스키 분해 # 수정된 촐레스키 분해(modified Cholesky decomposition)는 다음과 같다.\n6.3.1. 동기 # 모형 $$ \\begin{equation} X = \\begin{pmatrix} X_1 \\\\ \\vdots \\\\ X_k \\end{pmatrix} \\sim N(0, \\Sigma) \\end{equation} $$ 에서, 각 성분의 분포를 다음과 같이 나타낼 수 있다.\n$$ \\begin{equation} \\begin{aligned} X_1 \u0026amp;= \\epsilon_1, \\\\ X_2 \u0026amp;= a_{21} X_1 + \\epsilon_2, \\\\ X_3 \u0026amp;= a_{31} X_1 + a_{32} X_2 + \\epsilon_3, \\\\ \u0026amp;\\vdots \\\\ X_k \u0026amp;= a_{k1} X_1 + \\cdots + a_{k, k-1} X_{k-1} + \\epsilon_k \\end{aligned} \\end{equation} $$\n즉, $X = AX + \\epsilon, ~ \\epsilon \\sim N(0, D)$와 같은 형태로 나타낼 수 있다. 여기서 $$ \\begin{equation} A = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ a_{21} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{k1} \u0026amp; a_{k2} \u0026amp; \\cdots \u0026amp; 0 \\end{bmatrix} \\end{equation} $$ 이고, 이러한 $A$를 촐레스키 인자라 부른다.\n그러면, $$ \\begin{equation} \\begin{gathered} (I - A) X = \\epsilon, \\\\ Var((I-A)X) = Var(\\epsilon), \\\\ (I-A) \\Sigma (I-A)\u0026rsquo; = D, \\\\ \\Sigma = (I-A)^{-1} D (I-A)\u0026rsquo;^{-1}, \\\\ \\Omega = (I-A) D^{-1} (I-A)\u0026rsquo; \\end{gathered} \\end{equation} $$\n즉, $\\Sigma$를 추정하는 공분산 추정 문제를, $A$와 $D$를 추정하는 선형회귀문제로 바꿀 수 있다.\n수정된 촐레스키 분해는 주로 사용된다.\n7. 빈도론 추정 # 7.1. 벌점함수 방법들 # $$ \\begin{equation} \\begin{aligned} \\hat{\\Sigma} \u0026amp;= \\arg\\min_{\\Sigma} \\left[ - l(\\Sigma) + \\lambda Pen(\\Sigma) \\right] \\\\ \u0026amp;= \\arg\\min_{\\Sigma} \\log |\\Sigma| + tr(\\Sigma^{-1} S_n) + \\sum_{i \u0026lt; j} P_\\lambda( \\sigma_{ij}) \\end{aligned} \\end{equation} $$\n주로 사용하는 벌점함수로는 다음이 있다.\n$P_\\lambda(\\theta) = \\lambda |\\theta|$ ($L_1$-penalty, LASSO penalty) $P_\\lambda(\\theta) = \\lambda^2 - (|\\theta| - \\lambda)^2 I(|\\theta| \u0026lt; \\lambda)$, (hard thresholding) $P_\\lambda\u0026rsquo;(\\theta) \\lambda I(|\\theta| \\leq \\lambda) + \\dfrac{(a \\lambda - \\theta)_+}{a - 1} I(|\\theta| \u0026gt; \\lambda),~ a\u0026gt;2$ 공분산 행렬의 역행렬을 계산하는 것이 비싸기 때문에, 다음과 같은 손실함수를 고려하기도 한다.\n$$ \\begin{equation} \\sum_{i, j} (s_{ij} - \\sigma_{ij})^2 + \\sum_{i \u0026lt; j} P_\\lambda( \\sigma_{ij}) \\end{equation} $$\n혹은, 다음과 같이 정밀도 행렬을 추정하는 문제를 고려하기도 한다.\n$$ \\begin{equation} \\hat{\\Omega} = \\arg\\min_{\\Omega} - \\log |\\Omega| + tr(\\Omega S_n) + \\sum_{i \u0026lt; j} P_\\lambda( \\omega_{ij}) \\end{equation} $$\n7.2. Lam \u0026amp; Fan (2009) # 빈도론의 대표적인 연구 결과를 소개한다.\nLam \u0026amp; Fan (2009)은 적당한 벌점함수에 대해 $$|\\hat{\\Sigma} - \\Sigma_0|_F^2 = O_p\\left( \\frac{(p_n + s_n) \\log p_n}{n} \\right)$$ 을 보였다. 여기서 $s_n$은 $\\Sigma$에서 0이 아닌 비대각원소의 개수, $p_n$은 차원을 의미한다.\n보통, 공분산 행렬의 추정 분제에서는 최적의 수렴속도가 다음과 같이 주어진다.\n$$\\frac{\\text{0이 아닌 모수의 개수} \\times \\log(\\text{차원})}{n}$$\n빈도론자들은 이러한 $\\hat{\\Sigma}$를 찾는 구체적인 방법들에 대해 관심을 갖는다.\n7.3. 성김 가정이 없는 추정 방법들 # 최근에는 주로 성김 가정을 하나, 이전에는 어떤 추정 방법들을 제안했나 살펴본다.\n7.3.1. Stein (1975) # $S = PDP\u0026rsquo;$와 같이 나타내자. 적당한 고유치들의 변환 $\\Lambda$에 대해 공분산 행렬의 추정량으로 $\\hat{\\Sigma} = P \\Lambda(D) P\u0026rsquo;$로 제안한다.\nJohnstone의 문제에서 알 수 있듯, 고차원 행렬 문제에서는 고유벡터를 찾는 것도 어렵기 때문에 $P$를 제대로 추정하기 어렵다.\n7.3.2. Ledoit \u0026amp; Wolf (2004) # 공분산 행렬의 추정량으로 축소 추정량 $\\hat{\\Sigma} = \\rho_1 S + \\rho_2 I$를 제안하였다.\n7.4. 성김 가정 하에서의 빈도론 추정 방법들 # Bickel \u0026amp; Levina (2008) Thresholidng, Tapering, Banding\nThresholding estimator는 다음과 같이 주어진다. $$\\begin{equation} \\begin{aligned} \\hat{\\Sigma} \u0026amp;= (\\hat{\\sigma_{ij}}) \\\\ \\hat{\\sigma}{ij} \\\\ \u0026amp;= \\begin{cases} s{ij} I\\left(|s_{ij}| \u0026gt; c \\sqrt{ \\frac{\\log p}{n}}\\right) \u0026amp; (i \\neq j) \\\\ s_{ij} \u0026amp; (i=j) \\end{cases} \\end{aligned} \\end{equation}$$\nbanding estimator는 공분산 행렬이 대각성분 근처에서만 0이 아닌 성분을 갖는 추정량을 제안한다. $$\\begin{equation} \\hat{\\Sigma} = B_k(S) = (s_{ij} I(|i-j| \\leq k)) \\end{equation}$$\ntapering estimator는 공분산 행렬이 대각성분에서 멀어질수록 0에 가까워지는 추정량을 제안한다. \\begin{equation} \\hat{\\Sigma} = T_k(S) = ( w_{ij}^{(k)} s_{ij}), \\quad w_{ij}^{(k)} = \\begin{cases} 1, \u0026amp; |i-j| \\leq \\frac{k}{2} \\\\ 2 - \\frac{|i-j|}{k/2}, \u0026amp; \\frac{k}{2} \u0026lt; |i-j| \\leq k \\\\ 0, \u0026amp; \\text{o.w.} \\end{cases} \\end{equation}\n8. 베이즈 방법 # 모수공간에 제약이 있을 때, 사전분포를 부여하는 것이 어렵다.\n8.1. 그래프 모형 # 8.1.1. 그래프 # $G = (V, E)$라 하자. $\\Omega = (\\omega_{ij})$와 같이 나타낼 때, $V = { 1,2, \\cdots, k }$, $$E \\subset V \\times V = { (i, j) : Cov(X_i, X_j) \\neq 0 \\text{ or } w_{ij} \\neq 0 }$$ 으로 정의한다.\n8.1.2. G-Wishart 분포 # $\\Omega \\sim W_G(b, D), b \u0026gt; 2, D \u0026gt; 0$는 다음을 의미한다.\n$$\\pi(\\Omega | G) = \\frac{1}{I_G(b, D)} | \\Omega|^{\\frac{b-2}{2}} e^{-\\frac{1}{2} tr(D \\Omega)} I(\\Omega \\in M_G^+)$$\n여기서 $M_G^+ = { \\Omega : \\Omega \u0026gt; 0,~ \\omega_{ij} \\neq 0 \\Leftrightarrow (i, j)\\in E }$이다.\n사후분포는 $\\Omega | \\mathbb{X}, G \\sim W_G(b+n, D+S)$로 주어진다.\n이 분포는 단순히 위샤트 분포에 제약조건을 추가한 것이라 직관적이나, 정규화 상수 $I_G(b, D)$의 계산이 사실상 불가능하다.\n이러한 문제로 분해가능(decomposible)이라는 가정을 추가한다. 분해가능하지 않을 때는 수치적으로 정규화 상수를 계산하나 차원이 커질 때 계산이 거의 불가능하다.\n8.1.3. 그래프 모형 # 이와 같은 모형을 그래프 모형(graphical model)이라 한다.\n그래프 모형에서 $w_{ij} = 0$은 $X_i \\perp X_j|X_{~(i,j)}$, 즉, 조건부 독립성을 의미한다.\n참고: $\\sigma_{ij} = 0$은 $X_i \\perp X_j$, 즉, 주변 독립성을 의미한다.\n8.1.4. G-inverse-Wishart 분포 # $\\Sigma \\sim IW_G(\\delta, U)$는 다음과 같은 밀도함수를 갖는다.\n$$\\pi(\\Sigma | G) = \\frac{1}{I_G(\\delta, U)} | \\Sigma|^{-\\frac{\\delta+2}{2}} e^{-\\frac{1}{2} tr(\\Sigma^{-1} U)} I(\\Sigma \\in M_G^+)$$\n8.1.5. 성질 # 그래프 모형은 사전분포와 사후분포가 잘 정의된다는 장점을 갖는다.\n8.2. 축소 사전분포 # 8.2.1. Wang (2015) # 공분산 행렬의 각 성분에 다음과 같은 분포를 가정하는 모형도 있다.\n$$ \\begin{equation} \\begin{aligned} \\sigma_{ij} \u0026amp;\\sim w \\delta_0 + (1-w) Normal, \\\\ \\sigma_{ii} \u0026amp; \\sim Exp \\end{aligned} \\end{equation} $$\n우리가 이번에 볼 논문은 이를 연속형 분포로 확장한 것이다.\n8.3. 사후처리 사후분포 (이광민) # post-processed posterior\n사이비 베이즈(?)\n전체 모수 공간을 $\\Theta^\\ast$, 원하는 모수 공간을 $\\Theta \\subset \\Theta^\\ast$라 하자.\n사전분포 $\\pi^\\ast$가 계산이 쉬운 사후분포 $\\pi^\\ast(\\cdot | \\mathbb{X})$를 갖는다고 하자.\n사후처리 사후분포는 다음과 같은 요소들로 구성된다.\n사후 처리 함수 $f: \\Theta^\\ast \\rightarrow \\Theta$ 사후처리 사후분포 $[f(\\theta^\\ast)|\\theta^\\ast \\sim \\pi^\\ast(\\cdot | \\mathbb{X}_n)] = \\pi(\\cdot | \\mathbf{X})$ 이 방법은 이론적 정당성을 더 확보해야 한다.\n8.4. Berger, Sun \u0026amp; Song (2020) # 다음과 같은 사전분포를 고려한다.\n$$ \\begin{equation} \\pi(\\Sigma|a, b, H) \\propto \\frac{1}{|\\Sigma|^a \\left[ \\prod_{i \u0026lt; j} (\\lambda_i - \\lambda_j) \\right]^b } e^{-\\frac{1}{2} tr(\\Sigma^{-1} H)} \\end{equation} $$\n여기서 $\\lambda_1 \u0026gt; \\lambda_2 \u0026gt; \\cdots \u0026gt; \\lambda_k$는 $\\Sigma$의 고유치이다.\n사후분포의 성질은 아직 규명되지 않았다.\n많은 곳에서는 모수의 차원으로 $p$를 사용하나 여기서는 $k$를 사용한다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":1,"href":"/2022-Q2-workshop/docs/02-sicho/","title":"공분산 행렬의 베이즈 추론","section":"2022 Q2 Bayes Workshop","content":" 모형 # $$X_1, \\cdots, X_n | \\Sigma \\sim N(0, \\Sigma)$$ $$\\Sigma = (\\sigma_{ij})_{i,j=1}^p \u0026gt; 0$$\n사전분포 # 비대각원소 # 비대각원소에는 정규혼합 사전분포를 가정한다.\n$$\\pi(\\sigma_{ij}) = (1 - \\pi) N(\\sigma_{ij}; 0, v_0^2) + \\pi N(\\sigma_{ij}; 0, v_1^2), \\quad i \\neq j$$\n대각원소 # 대각원소에는 축소 사전분포를 가정한다.\n$$\\pi(\\sigma_{ii}) = Exp(\\lambda/2)$$\n공분산 행렬 # 이를 정리하면 다음과 같다.\n$$ \\begin{equation} \\begin{aligned} \\pi(\\Sigma) \u0026amp;= \\left[ c(\\theta) \\right]^{-1} \\prod_{i \u0026lt; j} \\left[ (1 - \\pi) N(\\sigma_{ij}; 0, v_0^2) + \\pi N(\\sigma_{ij}; 0, v_1^2) \\right]\\\\ \u0026amp;\\qquad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\end{aligned} \\end{equation} $$\n여기서 $c(\\theta)$는 정규화 상수이며 모수 $\\theta = \\{v_0, v_1, \\pi, \\lambda \\}$이다.\n변수 # $Z = (z_{ij})_{i \u0026lt; j} \\in \\{0, 1 \\}^{\\frac{p(p-1)}{2}}$\n계층모형 # 이 모형은 다음과 같은 계층모형으로 나타낼 수 있다.\n\\begin{equation} \\begin{aligned} \\pi(\\Sigma|Z, \\theta) \u0026amp;= \\left[ c(\\theta) \\right]^{-1} \\prod_{i \u0026lt; j} N(\\sigma_{ij}; 0, v_{z_{ij}}^2 ) \\\\ \u0026amp;\\qquad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\\\ \\pi(Z|\\theta) \u0026amp;= \\left[ c(\\theta) \\right]^{-1}c(z, v_0, v_1, \\lambda) \\prod_{i \u0026lt; j } \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}}. \\end{aligned} \\end{equation}\nBlock-Giibs Sampler # 위의 계층모형에서 표본을 추출하는 깁스 샘플러는 다음과 같다.\n\\begin{equation} \\begin{aligned} \\pi(\\Sigma, Z|X_1, \\cdots, X_n) \u0026amp;\\propto \\prod_{i=1}^n N_p(X_i; 0, \\Sigma) \\\\ \u0026amp;\\quad \\times \\prod_{i \u0026lt; j} N(\\sigma_{ij}; 0, v_{z_{ij}}^2 ) \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}} \\\\ \u0026amp;\\quad \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2}\\right) I(\\Sigma \\in M^+) \\\\ \u0026amp;\\propto |\\Sigma|^{-\\frac{n}{2}} \\exp\\left[ -\\frac{1}{2} tr(S\\Sigma^{-1}) \\right] \\\\ \u0026amp;\\quad \\times \\prod _ {i \u0026lt; j} \\left\\{ \\exp\\left( - \\frac{\\sigma_{ij}^2}{2v_{z_{ij}}^2} \\right) \\right \\} \\pi^{z_{ij}} (1-\\pi)^{1-z_{ij}} \\\\ \u0026amp;\\quad \\times \\prod_{i=1}^p \\exp\\left( -\\frac{\\lambda}{2} \\sigma_{ii} \\right) \\end{aligned} \\end{equation}\n$$P(z_{ij} = 1|\\Sigma, X_1, \\cdots, X_n) = \\frac{\\pi N(\\sigma_{ij}; 0, v_1^2)}{\\pi N(\\sigma_{ij} 0, v_1^2) + (1- \\pi) N(\\sigma_{ij}; 0, v_0^2)}$$\n$V = (v_{z_{ij}}^2)$은 $p \\times p$ 대칭 행렬, $v_{z_{ij}}^2 = 0$ for $i = j$.\n$$\\Sigma = \\begin{pmatrix} \\Sigma_{11} \u0026amp; \\sigma_{12} \\\\ \\sigma_{12}^\\prime \u0026amp; \\sigma_{22} \\end{pmatrix}$$\n$$ S = X^\\prime X = \\begin{pmatrix} S_{11} \u0026amp; s_{12} \\\\ s_{12}^\\prime \u0026amp; s_{22} \\end{pmatrix}$$\n$$V = \\begin{pmatrix} V_{11} \u0026amp; v_{12} \\\\ v_{12}^\\prime \u0026amp; 0 \\end{pmatrix}$$\n이제 다음과 같은 변환을 생각하자. $$(\\sigma_{12},~\\sigma_{22}) \\mapsto (u=\\sigma_{12},~v=\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12})$$\n이 변환의 야코비안은 다음과 같이 계산된다.\n\\begin{equation} |J| = \\left|\\begin{pmatrix} 1 \u0026amp; 0 \\\\ -2\\Sigma_{11}^{-1} \\sigma_{12} \u0026amp; 1 \\end{pmatrix}\\right| = 1 \\end{equation}\n그러면, 공분산의 역행렬은 \\begin{equation} \\begin{aligned} \\Sigma^{-1} \u0026amp;= \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} \\sigma_{12} \\left( \\sigma_{22} - \\sigma_{21}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}\\right) \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \u0026amp; \u0026hellip; \\\\ -(\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1}\\sigma_{12})^{-1}\\sigma_{12}^\\prime \\Sigma_{11}^{-1} \u0026amp; \\left(\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}\\right)^{-1} \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} u u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; -\\Sigma_{11}^{-1} uv^{-1} \\\\ -u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; v^{-1} \\end{pmatrix} \\end{aligned} \\end{equation}\n따라서, \\begin{equation} \\begin{aligned} |\\Sigma| \u0026amp;= |\\Sigma_{11}| |\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12} | \\\\ \u0026amp;= |\\Sigma_{11}| (\\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1}\\sigma_{12}) \\\\ \u0026amp;\\propto v, \\\\ tr(S\\Sigma^{-1}) \u0026amp;= tr\\left[ \\begin{pmatrix} S_{11} \u0026amp; s_{12} \\\\ s_{21}^\\prime \u0026amp; s_{22} \\end{pmatrix} \\begin{pmatrix} \\Sigma_{11}^{-1} + \\Sigma_{11}^{-1} u u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; -\\Sigma_{11}^{-1} uv^{-1} \\\\ -u^\\prime \\Sigma_{11}^{-1} v^{-1} \u0026amp; v^{-1} \\end{pmatrix} \\right] \\\\ \u0026amp;\\propto u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u v^{-1} -2 s_{12}^\\prime \\Sigma_{11}^{-1} u v^{-1} + s_22 v^{-1} \\end{aligned} \\end{equation}\n또한, \\begin{equation} \\prod_{i \u0026lt; j} \\exp\\left( - \\frac{\\sigma_j^2}{2 v_{z_{ij}}^2} \\right) \\propto \\exp\\left( - \\frac{1}{2} u^\\prime D^{-1} v \\right), \\end{equation} 여기서 $D = diag(v_{12}),~ v = \\sigma_{22} - \\sigma_{12}^\\prime \\Sigma_{11}^{-1} \\sigma_{12}$이다. \\begin{equation} \\prod_{i=1}^p \\exp\\left( - \\frac{\\lambda}{2} \\sigma_{ii} \\right) \\propto \\exp\\left( - \\frac{\\lambda}{2} \\left( u^\\prime \\Sigma_{11}^{-1} u + v \\right) \\right) \\end{equation} 에서,\n\\begin{equation} \\log \\pi(u, v | \\cdot ) \\propto -\\frac{1}{2} \\left \\{ n \\log v + u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u v^{-1} - 2 s_{12}^\\prime \\Sigma_{11}^{-1} u v^{-1} + s_{22} v^{-1} + u^\\prime D^{-1} u + \\lambda u^\\prime \\Sigma_{11}^{-1} u + \\lambda v \\right \\}, \\end{equation}\n\\begin{equation} \\begin{gathered} \\pi(u|v, z, \u0026hellip;) = N \\left( (B+D^{-1})^{-1} w, (B+D^{-1})^{-1} \\right), \\\\ B = \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} v^{-1} + \\lambda \\Sigma_{11}^{-1}, \\\\ w = \\Sigma_{11}^{-1} s_{12} v^{-1}. \\end{gathered} \\end{equation}\n이는 Generalized inverse Gaussian, $GIG(q, a, b)$로, 그 확률밀도함수는 \\begin{equation} f(x) = \\frac{(a/b)^{q/2}}{2K_q (\\sqrt{ab})} \\lambda^{p-1}e^{-(ax + b/x)/2} \\end{equation} 로 주어진다.\n즉, $\\pi(v|u, z, \u0026hellip;) = GIG\\left(1 - n/2, \\lambda, u^\\prime \\Sigma_{11}^{-1} S_{11} \\Sigma_{11}^{-1} u - 2 s_{12}^\\prime \\Sigma_{11}^{-1} u + s_{22}\\right)$이다.\n이 분포들에서 순서대로 표본을 추출하면 된다.\n참고: 실제로 이를 구현하면 수치적 오류로 인해 알고리듬이 잘 돌아가지 않는다.\n논문의 사전분포 # Armigan?\n$\\Sigma = (\\sigma_{ij})$ $\\rho = (\\rho_{ij})$ $\\pi(\\Sigma, \\rho) = \\prod_{i \u0026gt; j} N\\left( \\sigma_{ij}; 0, \\frac{\\rho_{ij}}{1-\\rho_{ij}} \\tau^2 \\right) Beta(\\rho_{ij}; a, b) \\times \\prod_{i=1}^p \\text{Exp}\\left(\\sigma_{ii}; \\frac{\\lambda}{2} \\right)$ $v = (v_{ij}^2) = \\begin{pmatrix} V_{11} \u0026amp; v_{12} \\\\ v_{12}^\\prime \u0026amp; 0 \\end{pmatrix},~ v_{ij}^2 = \\dfrac{\\rho_{ij}}{1 - \\rho_{ij}} \\tau^2$ $\\phi_{ij} = \\dfrac{\\rho_{ij}}{1 - \\rho_{ij}}$라 하자. 그러면, \\begin{equation} \\begin{aligned} \\sigma_{ij}|\\phi_{ij} \u0026amp;\\sim N(0, \\phi_{ij} \\tau^2), \\\\ \\phi_{ij}|\\psi_{ij} \u0026amp;\\sim \\text{Gamma}(a, \\psi_{ij}), \\\\ \\psi_{ij} \u0026amp;\\sim \\text{Gamma}(b, 1) \\end{aligned} \\end{equation} 에서 \\begin{equation} \\begin{aligned} \\pi(\\psi_{ij}|\\phi_{ij}, \u0026hellip;) \u0026amp;\\propto \\psi_{ij}^{b-1}e^{-\\psi_{ij}} \\phi_{ij}^{a-1} e^{-\\psi_{ij} \\phi_{ij}} \\psi_{ij}^a \\\\ \u0026amp;= \\psi_{ij}^{a+b-1} e^{- (\\phi_{ij} + 1)\\psi_{ij}} \\\\ \u0026amp;= \\text{Gamma}(\\cdot, a+b, \\phi_{ij}+ 1). \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} \\pi(\\phi_{ij}|\u0026hellip;) \u0026amp;\\propto \\phi_{ij}^{-\\frac{1}{2}}e^{-\\frac{\\sigma_{ij}^2}{2\\phi_{ij} \\tau^2}} \\phi_{ij}^{a-1} e^{-\\psi_{ij} \\phi_{ij}} \\\\ \u0026amp;= \\phi_{ij}^{a-\\frac{1}{2}-1}e^{-\\frac{\\sigma_{ij}^2}{2\\phi_{ij} \\tau^2} - \\psi_{ij} \\phi_{ij}} \\\\ \u0026amp;= GIG\\left(a - \\frac{1}{2},~ 2\\psi_{ij},~ \\frac{\\sigma_{ij}^2}{\\tau^2}\\right) \\end{aligned} \\end{equation}\n논문의 의의 # 축소 사전분포의 사용 이론적 성질의 규명 "},{"id":2,"href":"/2022-Q2-workshop/docs/03/","title":"Theorem 3 and Lemma 2","section":"2022 Q2 Bayes Workshop","content":"TBA\n"},{"id":3,"href":"/2022-Q2-workshop/docs/04/","title":"Theorem 4 and Lemma 3","section":"2022 Q2 Bayes Workshop","content":"TBA\n"},{"id":4,"href":"/2022-Q2-workshop/docs/05/","title":"Lemma 4 and Theorem 5","section":"2022 Q2 Bayes Workshop","content":"$\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}$ $\\newcommand{\\abs}[1]{\\left\\lvert #1 \\right\\rvert}$ $\\newcommand{\\Uc}{\\mathcal{U}}$ $\\newcommand{\\Cc}{\\mathcal{C}}$ $\\newcommand{\\eps}{\\epsilon}$ $\\newcommand{\\Real}{\\mathbb{R}}$\n[Lemma 3 in the paper] If $\\Sigma_0 \\in \\Uc(s_0, \\zeta_0)$ and $\\Sigma\\in \\Uc(\\zeta)$ then we have\n$K(f_{\\Sigma_0}, f_\\Sigma)\\leq \\zeta^4\\zeta_0^2 \\norm{\\Sigma-\\Sigma_0}_F^2 $ $V(f_{\\Sigma_0}, f_\\Sigma)\\leq \\frac 32 \\zeta^4\\zeta_0^2 \\norm{\\Sigma-\\Sigma_0}_F^2 $ [Lemma 4 in the paper] If $a=b=1/2$ , $x\u0026gt;1$, and $\\tau/x \u0026gt;0$ is sufficiently small then $\\pi_{ij}^u(x)\\geq \\sqrt{\\frac{1}{2\\pi}} \\frac{\\tau}{x^2}$ where $\\pi_{ij}^u(\\sigma_{ij})$ is the unconstrained marginal prior density of $\\sigma_{ij}$\n[Theorem 5 in the paper ; The lower bound for $\\pi(B_{\\eps_n})$] Here are the conditions we need for this theorem. $\\Sigma_0 \\in \\Uc(s_0, \\zeta_0)$ with $\\zeta_0 \u0026lt;\\zeta$ $p \\asymp n^\\beta$ for some $0\u0026lt;\\beta\u0026lt;1$ $\\zeta^4\\leq p$ $\\zeta^2\\zeta_0^2 \\leq s_0\\log p$ $n \\geq \\max \\{1/\\zeta_0^4,s_0/ (1-\\zeta_0/\\zeta)^2 \\} \\log p/\\zeta^4$ $p^{-1}\u0026lt;\\lambda \u0026lt; \\log p/\\zeta_0$ $a=b=1/2$ $(p^2\\sqrt{n})^{-1}\\lesssim \\tau \\lesssim (p^2\\sqrt{n})^{-1}\\sqrt{s_0\\log p}$ (Additional, From Thm 1 at page 5 of the paper) $(p+s_0)\\log p = o(n)$ i.e. $\\eps_n^2 \\rightarrow 0$ (Additional, From page 4 of the paper) $p = O(s_0)$ If the conditions above hold, then we have $\\pi(B_{\\eps_n})\\geq \\exp\\Big\\{-(5+\\frac 1\\beta )n\\eps_n^2 \\Big\\}$\n[Proof of Lemma 4] Because we have $a=b=1/2$, $$\\sigma_{ij}|\\rho_{ij} \\sim N(0, \\frac{\\rho_{ij}}{1-\\rho_{ij}}\\tau^2) \\;,\\; \\rho_{ij} \\sim \\text{Beta}(a,b) $$ is equivalent to $$\\sigma_{ij}| \\lambda_{ij} \\sim N(0, \\lambda_{ij}^2\\tau^2)\\;,\\; \\lambda_{ij} \\sim \\text{C}^+(0, 1)$$ where $\\text{C}^+(0, s)$ denotes the standard half-Cauchy distribution on positive real with a scale parameter $s$.\n$$\\begin{equation*} \\begin{aligned} p(\\sigma, \\rho) \u0026amp;= p(\\sigma | \\rho) p(\\rho) = \\frac{1}{\\sqrt{2\\pi \\frac{\\rho}{1-\\rho}\\tau^2}}\\exp\\Big(-\\frac{1}{2\\frac{\\rho}{1-\\rho}\\tau^2}\\sigma^2 \\Big) \\frac{1}{\\pi}\\rho^{-1/2}(1-\\rho)^{-1/2} \\quad \\because \\; \\Gamma(1/2) = \\sqrt{\\pi} \\end{aligned} \\end{equation*}$$\n$$\\begin{equation*}\\begin{aligned} \\lambda \u0026amp;= \\sqrt{\\frac{\\rho}{1-\\rho}} \\quad (\\lambda\u0026gt;0)\\quad \\text{and} \\quad \\rho = \\frac{\\lambda^2}{\\lambda^2+1} \\\\ \\text{Jacobian} \u0026amp;= \\abs{\\frac{d\\rho}{d\\lambda}} = \\frac{2\\lambda}{(\\lambda^2+1)^2} \\end{aligned}\\end{equation*}$$\n$$\\begin{equation*}\\begin{aligned} p(\\sigma, \\lambda) \u0026amp;= \\frac{1}{\\sqrt{2\\pi \\lambda^2 \\tau^2}}\\exp\\Big(-\\frac{1}{2\\lambda^2 \\tau^2}\\sigma^2 \\Big) \\frac{1}{\\pi} \\sqrt{\\frac{\\lambda^2}{\\lambda^2+1} \\frac{1}{\\lambda^2+1}}^{-1} \\frac{2\\lambda}{(\\lambda^2+1)^2} \\\\ \u0026amp;= \\frac{1}{\\sqrt{2\\pi \\lambda^2 \\tau^2}}\\exp\\Big(-\\frac{1}{2\\lambda^2 \\tau^2}\\sigma^2 \\Big) \\frac{2}{\\pi} \\frac{\\lambda^2+1}{\\lambda} \\frac{\\lambda}{(\\lambda^2+1)^2} \\\\ \u0026amp;= \\frac{1}{\\sqrt{2\\pi \\lambda^2 \\tau^2}}\\exp\\Big(-\\frac{1}{2\\lambda^2 \\tau^2}\\sigma^2 \\Big) \\frac{2}{\\pi} \\frac{1}{(\\lambda^2+1)} \\\\ \u0026amp;= p(\\sigma|\\lambda)p(\\lambda) \\end{aligned}\\end{equation*}$$\nHence we can conclude that $$\\sigma | \\rho \\sim N(0, \\frac{\\rho}{1-\\rho}\\tau^2) \\;, \\; \\rho \\sim \\text{Beta}(1/2, 1/2)$$ is equivalent to $$\\sigma |\\lambda \\sim N(0, \\lambda^2 \\tau^2)\\;, \\; \\lambda\\sim C^+(0,1)$$\nNow we shall derive tight bound for marginal prior density of $\\sigma$ $$\\begin{equation*}\\begin{aligned} \\pi^u(\\sigma) \u0026amp;= \\int_0^\\infty p(\\sigma, \\lambda) , d\\lambda \\\\ \u0026amp;= \\int_0^\\infty \\frac{1}{\\sqrt{2\\pi \\lambda^2 \\tau^2}}\\exp\\Big(-\\frac{1}{2\\lambda^2 \\tau^2}\\sigma^2 \\Big) \\frac{2}{\\pi} \\frac{1}{(\\lambda^2+1)} , d\\lambda \\\\ \u0026amp; \\text{change of variable} : u = 1 / \\lambda^2 \\Leftrightarrow \\lambda = u^{-1/2} \\quad d\\lambda = -\\frac{1}{2} u^{-3/2}, du \\\\ \u0026amp;= \\int_0^\\infty \\frac{1}{\\sqrt{2\\pi \\tau^2}} u^{1/2}\\exp\\Big(-\\frac{1}{2\\tau^2}\\sigma^2 u \\Big) \\frac{2}{\\pi}\\frac{u}{1+u} \\frac{1}{2}u^{-3/2}, du \\\\ \u0026amp;= \\int_0^\\infty \\frac{1}{\\sqrt{2\\pi^3 \\tau^2}} \\exp\\Big(-\\frac{\\sigma^2}{2\\tau^2} u \\Big) \\frac{1}{1+u} , du \\\\ \u0026amp; \\text{change of variable} : z = 1+u \\Leftrightarrow u = z-1 \\\\ \u0026amp;= \\frac{1}{\\tau \\sqrt{2\\pi^3}} \\exp\\Big(\\frac{\\sigma^2}{2\\tau^2} \\Big) \\int_1^\\infty \\frac{1}{z} \\exp\\Big(-\\frac{\\sigma^2}{2\\tau^2}z \\Big), dz \\end{aligned}\\end{equation*}$$\nDefine exponential integral $E_1$ as the following : $$E_1(x) = \\int_1^\\infty \\frac{1}{z}\\exp(-zx), dz \\quad \\forall \\; x\u0026gt;0 $$ Then it has tight bound given as $$\\frac{1}{2}\\exp(-x)\\log\\Big( 1+ \\frac{2}{x}\\Big) \u0026lt; E_1(x) \u0026lt; \\exp(-x)\\log\\Big(1+\\frac{1}{x} \\Big) \\quad x\u0026gt;0 $$ Note that this tight bound is mentioned in Wikipedia : Exponential integral\nThus, we have $$\\begin{equation*}\\begin{aligned} \\pi^u(\\sigma) \u0026amp;= \\frac{1}{\\tau \\sqrt{2\\pi^3}} \\exp\\Big(\\frac{\\sigma^2}{2\\tau^2} \\Big) E_1\\Big(\\frac{\\sigma^2}{2\\tau^2} \\Big) \\end{aligned}\\end{equation*}$$ Using the tight bound of $E_1$ given above, we get $$\\begin{equation*}\\begin{aligned} \\pi^u(\\sigma) \u0026amp;\u0026lt; \\frac{1}{\\tau \\sqrt{2\\pi^3}}\\log\\Big(1+\\frac{2\\tau^2}{\\sigma^2} \\Big) \\\\ \\pi^u(\\sigma) \u0026amp;\u0026gt; \\frac{1}{2\\tau \\sqrt{2\\pi^3}}\\log\\Big(1+\\frac{4\\tau^2}{\\sigma^2} \\Big) \\end{aligned}\\end{equation*}$$ From now on, we will call these two inequalities as upper and lower bound of marginal prior density of $\\sigma_{ij}$ respectively.\nThen, using lower bound of marginal prior density of $\\sigma_{ij}$, we have the following : $$\\begin{equation*}\\begin{aligned} \\pi_{ij}^u(x) \u0026amp;\\geq \\frac{1}{2\\tau}\\sqrt{\\frac{1}{2\\pi^3}}\\log\\Big(1+\\frac{4\\tau^2}{x^2}\\Big) \\\\ \u0026amp;\\geq \\frac{1}{4\\tau}\\sqrt{\\frac{1}{2\\pi^3}}\\frac{4\\tau^2}{x^2} \\quad \\because \\log(1+x)\\geq \\frac 12 x \\quad \\text{when} \\; 0\\leq x \\leq 1 \\quad \\text{and} \\quad \\tau/x \\; \\text{is suff. small} \\\\ \u0026amp;= \\sqrt{\\frac{1}{2\\pi^3}}\\frac{\\tau}{x^2} \\end{aligned}\\end{equation*}$$\n[Proof of Theorem 5] Note that $B_{\\eps}$ is defined as $B_\\eps = \\{f_\\Sigma : \\Sigma \\in \\Cc_p , \\; K(f_{\\Sigma_0}, f_\\Sigma)\u0026lt; \\eps^2 , \\; V(f_{\\Sigma_0}, f_\\Sigma)\u0026lt; \\eps^2 \\}$\nBy Lemma 3, it suffices to show that $\\pi\\Big(\\norm{\\Sigma- \\Sigma_0}_F^2 \\leq \\frac{2}{3\\zeta^4\\zeta_0^2}\\eps_n^2\\Big)\\geq \\exp(-Cn\\eps_n^2)$\nThis is because\n$$ \\begin{equation*} \\begin{aligned} \u0026amp;\\norm{\\Sigma- \\Sigma _ 0} _ F^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\eps _ n^2 \\\\ \u0026amp;\\Rightarrow K(f _ {\\Sigma _ 0, f _ \\Sigma}) \\leq \\zeta^4\\zeta _ 0^2 \\norm{\\Sigma - \\Sigma _ 0} _ F^2 \\leq \\frac 23 \\eps _ n^2 \u0026lt; \\eps _ n^2 \\quad \\text{and} \\quad V(f _ {\\Sigma _ 0}, f _ \\Sigma) \\leq \\frac 32 \\zeta^4\\zeta _ 0^2 \\norm{\\Sigma - \\Sigma _ 0} _ F^2 \\leq \\eps _ n^2 \\\\ \u0026amp;\\Rightarrow f _ \\Sigma \\in B _ {\\eps _ n} \\end{aligned} \\end{equation*} $$\nso that $\\pi(B_{\\eps_n}) \\geq \\pi\\Big(\\norm{\\Sigma- \\Sigma_0}_F^2 \\leq \\frac{2}{3\\zeta^4\\zeta_0^2}\\eps_n^2\\Big)$\nNote that $$ \\begin{equation*} \\begin{aligned} \u0026amp;\\pi\\Big(\\norm{\\Sigma- \\Sigma_0} _ F^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\eps _ n^2\\Big) = \\pi\\Big(\\norm{\\Sigma- \\Sigma _ 0} _ F^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2} \\frac{(p+s _ 0)\\log p}{n} \\Big) \\\\ \u0026amp;\\geq \\pi \\left( \\sum _ {i\\neq j}(\\sigma _ {ij} - \\sigma _ {ij} ^ \\ast)^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\frac{s _ 0\\log p}{n} \\;, \\; \\sum _ {j=1}^p(\\sigma _ {jj} - \\sigma _ {jj} ^ \\ast)^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\frac{p\\log p}{n} \\right) \\quad \\because \\; x\\leq \\alpha, y \\leq \\gamma \\Rightarrow x+y \\leq \\alpha + \\gamma \\\\ \u0026amp;\\geq \\pi\\Big(\\max _ {i\\neq j}(\\sigma _ {ij}- \\sigma _ {ij} ^ \\ast)^2\\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\frac{s _ 0\\log p}{p(p-1)n} \\;,\\; \\max _ {1\\leq j\\leq p}(\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta _ 0^2}\\frac{\\log p}{n} \\Big) := \\pi(A _ {n, \\Sigma_0}) \\end{aligned} \\end{equation*} $$ where $\\Sigma_0 = (\\sigma_{ij}^\\ast)$\nWe will introduce Weyl\u0026rsquo;s theorem here. (Source : Wikipedia : Weyl\u0026rsquo;s inequality)\nIf $A, B$ are $n\\times n$ symmetric (or Hermitian) matrices then $\\lambda_k(A) + \\lambda_n(B) \\leq \\lambda_k(A+B) \\leq \\lambda_k(A) +\\lambda_1(B)\\quad \\forall\\; k=1, \\cdots, n$ where $\\lambda_1(M)\\geq \\cdots \\geq \\lambda_n(M)$ are eigenvalues of symmetric matrix $M\\in \\Real^{n\\times n}$\nHere, we will plug in $A= \\Sigma_0$ , $B= \\Sigma - \\Sigma_0$ so that $A+B = \\Sigma$\nAlso, we will use two more properties about matrix norm.\nThe first one is that for symmetric A, we have $-\\norm{A}_2 \\leq \\lambda(A)\\leq \\norm{A}_2$ where $\\lambda(A)$ is any eigenvalue of $A$. This is because $\\lambda(A)^2 = \\lambda(A^2) = \\lambda(A^T A) \\Rightarrow \\abs{\\lambda(A)} = \\sqrt{\\lambda(A^T A)}\\leq \\norm{A}_2 $\nThe second one is the special case of the Hölder inequality $$\\norm{A} _ 2 \\leq \\sqrt{\\norm{A} _ 1\\norm{A} _ \\infty}$$ (Source : Wikipedia : Matrix Norm) Also, if $A$ is symmetric, then $$\\norm{A} _ 1 = \\norm{A} _ \\infty$$ since the former is maximum absolute column sum and the latter is maximum absolute row sum. Thus we get $\\norm{A}_2 \\leq \\norm{A}_1$ given $A$ is symmetric.\nWe want to show that $\\Sigma \\in A_{n, \\Sigma_0} \\Rightarrow \\Sigma \\in \\Uc(\\zeta)$\nSuppose $\\Sigma \\in A_{n, \\Sigma_0}$. Then we have $$\\norm{\\Sigma- \\Sigma_0} _ 1 \\leq (p-1)\\max_{i\\neq j}\\abs{\\sigma _ {ij}- \\sigma _ {ij} ^ \\ast} + \\max_{1\\leq j\\leq p}\\abs{\\sigma_{jj} - \\sigma_{jj}^\\ast}$$\n$$\\begin{equation*}\\begin{aligned} \\lambda_{min}(\\Sigma) \u0026amp;\\geq \\lambda_{min}(\\Sigma _ 0) + \\lambda _ {min}(\\Sigma-\\Sigma _ 0) \\quad \\because \\; \\text{Weyl\u0026rsquo;s thm} \\\\ \u0026amp;\\geq \\lambda _ {min}(\\Sigma _ 0) - \\norm{\\Sigma- \\Sigma _ 0} _ 2 \\quad \\because \\; -\\norm{A} _ 2 \\leq \\lambda(A) \\leq \\norm{A} _ 2 \\\\ \u0026amp;\\geq \\lambda _ {min}(\\Sigma _ 0) - \\norm{\\Sigma-\\Sigma _ 0} _ 1 \\quad \\because \\; \\norm{A} _ 2 \\leq \\norm{A} _ 1 \\; \\text{given } A \\text{ is symmetric } \\\\ \u0026amp;\\geq \\zeta _ 0^{-1} - \\Big\\{ (p-1)\\sqrt{ \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\frac{s _ 0\\log p}{p(p-1)n}} + \\sqrt{\\frac{2}{3\\zeta^4 \\zeta _ 0^2}\\frac{\\log p}{n} } \\Big\\} \\quad \\because \\; \\Sigma _ 0 \\in \\Uc(s _ 0, \\zeta _ 0)\\;,\\; \\Sigma \\in A _ {n, \\Sigma _ 0} \\\\ \u0026amp;:= \\zeta _ 0^{-1} - \\star \\\\ \u0026amp; \\\\ \\lambda _ {max}(\\Sigma) \u0026amp;\\leq \\lambda _ {max}(\\Sigma _ 0) + \\lambda _ {max}(\\Sigma- \\Sigma _ 0) \\\\ \u0026amp;\\leq \\lambda _ {max}(\\Sigma _ 0) + \\norm{\\Sigma - \\Sigma _ 0} _ 2 \\\\ \u0026amp;\\leq \\lambda _ {max}(\\Sigma _ 0) + \\norm{\\Sigma - \\Sigma _ 0} _ 1 \\\\ \u0026amp;\\leq \\zeta _ 0 + \\Big\\{ (p-1)\\sqrt{ \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\frac{s _ 0\\log p}{p(p-1)n}} + \\sqrt{\\frac{2}{3\\zeta^4 \\zeta _ 0^2}\\frac{\\log p}{n} } \\Big\\} \\\\ \u0026amp;= \\zeta _ 0 + \\star \\end{aligned}\\end{equation*}$$\nWe shall claim that $\\star \\rightarrow 0$ as $n\\rightarrow \\infty$\n$$\\star \\leq \\sqrt{\\frac{2}{3\\zeta^4\\zeta_0^2}} \\sqrt{\\frac{(s_0+1)\\log p}{n}} \\leq \\sqrt{\\frac{2}{3\\zeta^4\\zeta_0^2}} \\sqrt{\\frac{(s_0+p)\\log p}{n}} \\rightarrow 0 \\quad \\because \\; \\eps_n \\rightarrow 0 $$\nThus, combining the fact that $\\zeta_0 \u0026lt; \\zeta$ and $\\star \\rightarrow 0$, we get $$\\lambda_{min}(\\Sigma)\\geq \\zeta_0^{-1} - \\star \u0026gt; \\zeta^{-1} \\quad \\text{and} \\quad \\lambda_{max}(\\Sigma)\\leq \\zeta_0 + \\star \u0026lt; \\zeta \\quad \\text{for all suff. large } n$$\nHence, we have shown that $\\Sigma \\in A_{n, \\Sigma_0} \\Rightarrow \\Sigma \\in \\Uc(\\zeta)$ as desired.\nUsing above, we get $\\pi(A_{n, \\Sigma_0})\\geq \\pi^u(A_{n, \\Sigma_0})$ since $$\\pi(A_{n, \\Sigma_0}) = \\frac{\\pi^u(A_{n, \\Sigma_0})\\text{I}(\\Sigma\\in \\Uc(\\zeta))}{\\pi^u(\\Sigma\\in \\Uc(\\zeta))} = \\frac{\\pi^u(A_{n, \\Sigma_0})}{\\pi^u(\\Sigma\\in \\Uc(\\zeta))} \\geq \\pi^u(A_{n, \\Sigma_0}) \\quad \\because \\; \\pi^u(\\Sigma \\in \\Uc(\\zeta))\\leq 1$$\nHere, we shall briefly check what we have already shown. $$\\pi(B _ {\\eps _ n}) \\geq \\pi\\Big(\\norm{\\Sigma- \\Sigma _ 0} _ F^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\eps _ n^2\\Big) \\geq \\pi(A _ {n, \\Sigma _ 0})\\geq \\pi^u(A _ {n, \\Sigma _ 0}) $$\nHence, from now on, our goal is to prove that $\\pi^u(A_{n, \\Sigma_0})\\geq \\exp(-Cn\\eps_n^2)$\nNote that $$ \\begin{equation*}\\begin{aligned} \\pi ^ u(A _ {n, \\Sigma _ 0}) \u0026amp;= \\pi ^ u\\Big(\\max _ {i\\neq j}(\\sigma _ {ij}- \\sigma _ {ij} ^ \\ast) ^ 2\\leq \\frac{2}{3\\zeta ^ 4\\zeta _ 0 ^ 2}\\frac{s _ 0\\log p}{p(p-1)n} \\;,\\; \\max _ {1\\leq j\\leq p}(\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast) ^ 2 \\leq \\frac{2}{3\\zeta ^ 4 \\zeta _ 0 ^ 2}\\frac{\\log p}{n}\\Big) \\\\ \u0026amp;= \\pi ^ u\\Big(\\max _ {i\\neq j}(\\sigma _ {ij}- \\sigma _ {ij} ^ \\ast) ^ 2\\leq \\frac{2}{3\\zeta ^ 4\\zeta _ 0 ^ 2}\\frac{s _ 0\\log p}{p(p-1)n}\\Big) \\times \\pi ^ u\\Big(\\max _ {1\\leq j\\leq p}(\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast) ^ 2 \\leq \\frac{2}{3\\zeta ^ 4 \\zeta _ 0 ^ 2}\\frac{\\log p}{n} \\Big) \\\\ \u0026amp;= \\prod _ {i \u0026lt; j} \\pi ^ u\\Big((\\sigma _ {ij}- \\sigma _ {ij} ^ \\ast) ^ 2\\leq \\frac{2}{3\\zeta ^ 4\\zeta _ 0 ^ 2}\\frac{s _ 0\\log p}{p(p-1)n} \\Big) \\times \\prod _ {j=1} ^ p \\pi ^ u\\Big((\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast) ^ 2 \\leq \\frac{2}{3\\zeta ^ 4 \\zeta _ 0 ^ 2}\\frac{\\log p}{n} \\Big) \\end{aligned}\\end{equation*} $$\nThis is because all elements of $\\Sigma$ are independent to each other given unconstrained setting.\nObserve $$ \\prod _ {j=1}^p \\pi^u\\Big((\\sigma_{jj}- \\sigma_{jj}^\\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta_0^2}\\frac{\\log p}{n} \\Big) $$ first. We want to find a lower bound of this term.\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod _ {j=1} ^ p \\pi ^ u\\Big((\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast) ^ 2 \\leq \\frac{2}{3\\zeta ^ 4 \\zeta _ 0 ^ 2}\\frac{\\log p}{n} \\Big) = \\prod _ {j=1} ^ p \\pi ^ u\\Big(\\abs{\\sigma _ {jj}- \\sigma _ {jj} ^ \\ast} \\leq \\sqrt\\psi \\Big) \\quad \\text{where} \\; \\psi:= \\frac{2}{3\\zeta ^ 4 \\zeta _ 0 ^ 2}\\frac{\\log p}{n} \\\\ \u0026amp;= \\prod _ {j=1} ^ p \\pi ^ u(\\sigma _ {jj} ^ \\ast -\\sqrt \\psi \\leq \\sigma _ {jj}\\leq \\sigma _ {jj} ^ \\ast + \\sqrt \\psi ) \\quad \\because \\; \\psi \\rightarrow 0 \\; \\text{so that}\\; \\sigma _ {jj} ^ \\ast - \\sqrt \\psi \\geq 0 \\; \\text{for all suff. large } n \\\\ \u0026amp;\\text{Note that } \\sigma _ {jj}\\sim \\Gamma(1, \\lambda/2)\\; \\text{and}\\; \\pi ^ u(\\sigma _ {jj}) = \\frac{\\lambda}{2}\\exp(-\\frac{\\lambda}{2}\\sigma _ {jj}) \\; \\text{is decreasing function } \\\\ \u0026amp;\\geq \\prod _ {j=1} ^ p 2\\sqrt\\psi , \\pi ^ u(\\sigma _ {jj} ^ \\ast + \\sqrt \\psi) = \\prod _ {j=1} ^ p 2\\sqrt\\psi \\frac \\lambda 2 \\exp(-\\frac \\lambda 2 (\\sigma _ {jj} ^ \\ast + \\sqrt \\psi )) = \\prod _ {j=1} ^ p \\sqrt\\psi \\lambda \\exp(-\\frac \\lambda 2 (\\sigma _ {jj} ^ \\ast + \\sqrt \\psi )) \\\\ \u0026amp;\\geq \\prod _ {j=1} ^ p \\sqrt\\psi \\lambda \\exp(-\\frac \\lambda 2 (\\zeta _ 0 + \\sqrt \\psi )) \\quad \\because \\; \\sigma _ {jj} ^ \\ast \\leq \\lambda _ {max}(\\Sigma _ 0)\\leq \\zeta _ 0 \\quad \\text{due to energy boundedness} \\\\ \u0026amp;= \\Big\\{\\sqrt\\psi \\lambda \\exp(-\\frac \\lambda 2 (\\zeta _ 0 + \\sqrt \\psi )) \\Big\\} ^ p \\end{aligned}\\end{equation*}$$\nUsing a condition $\\log p/\\zeta ^ 4 \\zeta _ 0 ^ 4 \\leq n$ , we have $\\lambda\\sqrt \\psi \\leq \\lambda \\zeta _ 0$ since $$\\lambda \\sqrt{\\psi} = \\lambda \\sqrt{\\frac{2}{3\\zeta ^ 4 \\zeta_0^2}\\frac{\\log p}{n}} = \\lambda \\zeta_0 \\sqrt{\\frac{2}{3\\zeta^4 \\zeta_0^4}\\frac{\\log p}{n}} \\leq \\lambda \\zeta_0 \\sqrt{\\frac 23}\\leq \\lambda \\zeta_0$$\nHence, we can proceed the above inequality as the following : $$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{j=1}^p \\pi^u\\Big((\\sigma_{jj}- \\sigma_{jj}^\\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta_0^2}\\frac{\\log p}{n} \\Big) \\geq \\Big\\{\\sqrt\\psi \\lambda \\exp(-\\frac \\lambda 2 (\\zeta_0 + \\sqrt \\psi )) \\Big\\}^p \\\\ \u0026amp;= \\exp(p\\log \\lambda \\sqrt \\psi)\\exp\\Big(- \\frac \\lambda 2 p \\zeta_0 - \\frac \\lambda 2 p \\sqrt \\psi \\Big) \\\\ \u0026amp;\\geq \\exp(p\\log \\lambda \\sqrt \\psi)\\exp\\Big(- \\frac \\lambda 2 p \\zeta_0 - \\frac \\lambda 2 p \\zeta_0 \\Big) \\quad \\because \\; \\lambda\\sqrt \\psi \\leq \\lambda \\zeta_0 \\\\ \u0026amp;= \\exp\\Big(- p\\lambda \\zeta_0 - p \\log \\frac{1}{\\lambda\\sqrt{\\psi}}\\Big) \\\\ \u0026amp;\\geq \\exp\\Big(-p \\log p - p \\log \\frac{1}{\\lambda\\sqrt{\\psi}} \\Big) \\quad \\because \\; \\lambda \u0026lt; \\log p / \\zeta_0 \\; \\text{by assumption} \\end{aligned}\\end{equation*}$$\nHere, we shall claim that $1/\\sqrt \\psi \\leq \\zeta^3 p^{1/2\\beta}$ for all sufficiently large $n$\n$$\\begin{equation*}\\begin{aligned} 1/\\sqrt \\psi \u0026amp;= \\sqrt{\\frac 32}\\zeta_0 \\zeta^2 \\sqrt{\\frac{n}{\\log p}} \\\\ \u0026amp;\u0026lt; \\sqrt{\\frac 32}\\zeta^3 \\sqrt{\\frac{n}{\\log p}} \\quad \\because \\zeta_0 \u0026lt; \\zeta \\; \\text{by assumption} \\\\ \u0026amp;\\leq \\sqrt{\\frac 32}\\zeta^3 Cp^{1/2\\beta} \\frac{1}{\\sqrt{\\log p}} \\quad \\because \\; p \\asymp n^\\beta ,,, n^\\beta\\leq Cp \\text{ for some } C\u0026gt;0 \\text{ by assumption } \\\\ \u0026amp;\\leq \\zeta^3p^{1/2\\beta} \\quad \\because \\; p \\; \\text{gets large enough to attain} \\; \\sqrt{3/2}C/\\sqrt{\\log p}\u0026lt;1 \\end{aligned}\\end{equation*}$$\nWe will complete our process of finding lower bound of $$\\prod_{j=1}^p \\pi^u\\Big((\\sigma_{jj}- \\sigma_{jj}^\\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta_0^2}\\frac{\\log p}{n} \\Big)$$ as the below.\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{j=1}^p \\pi^u\\Big((\\sigma_{jj}- \\sigma_{jj}^\\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta_0^2}\\frac{\\log p}{n} \\Big) \\geq \\exp\\Big(-p \\log p - p \\log \\frac{1}{\\lambda\\sqrt{\\psi}} \\Big) \\\\ \u0026amp;\\geq \\exp\\Big(-p\\log p - p\\log \\frac{\\zeta^3 p^{1/2\\beta}}{\\lambda} \\Big) \\quad \\because \\; 1/\\sqrt \\psi \\leq \\zeta^3 p^{1/2\\beta} \\; \\text{for all sufficiently large } n \\\\ \u0026amp;\\geq \\exp\\Big(-p \\log p - p (1 + \\frac 34 + \\frac{1}{2\\beta})\\log p\\Big) \\quad \\because \\; p^{-1}\u0026lt;\\lambda \\; \\text{and} \\; \\zeta^4\\leq p \\quad \\text{by assumption} \\\\ \u0026amp;\\geq \\exp\\Big(-(3+\\frac{1}{2\\beta})p \\log p\\Big) \\end{aligned}\\end{equation*}$$\nHence we have $$ \\begin{equation} \\label{diagonal ineq} \\prod_{j=1}^p \\pi^u\\Big((\\sigma_{jj}- \\sigma_{jj}^\\ast)^2 \\leq \\frac{2}{3\\zeta^4 \\zeta_0^2}\\frac{\\log p}{n} \\Big) \\geq \\exp\\Big(-(3+\\frac{1}{2\\beta})p \\log p\\Big) \\end{equation} $$ for sufficiently large $n$.\nNext, we shall find a lower bound of $$\\prod_{i \u0026lt; j} \\pi^u\\Big((\\sigma_{ij}- \\sigma_{ij}^\\ast)^2\\leq \\frac{2}{3\\zeta^4\\zeta_0^2}\\frac{s_0\\log p}{p(p-1)n} \\Big).$$ Note that it can be decomposed as the following.\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{i \u0026lt; j} \\pi^u\\Big((\\sigma_{ij}- \\sigma_{ij}^\\ast)^2\\leq \\frac{2}{3\\zeta^4\\zeta_0^2}\\frac{s_0\\log p}{p(p-1)n} \\Big) = \\prod_{i\u0026lt;j} \\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi) \\quad \\text{where} \\; \\phi = \\frac{2}{3\\zeta^4\\zeta_0^2}\\frac{s_0\\log p}{p(p-1)n} \\\\ \u0026amp;= \\prod_{(i,j)\\in s(\\Sigma_0)}\\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi) \\times \\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j}\\pi^u(\\abs{\\sigma_{ij}}\\leq \\sqrt \\phi) \\end{aligned}\\end{equation*}$$\nBefore finding the lower bound of those two terms above, recall the tight bound of marginal prior density of off diagonal $\\sigma_{ij}$ of covariance matrix. $$\\begin{equation*}\\begin{aligned} \\pi^u(\\sigma_{ij}) \u0026amp;\u0026lt; \\frac{1}{\\tau \\sqrt{2\\pi^3}}\\log\\Big(1+\\frac{2\\tau^2}{\\sigma_{ij}^2} \\Big) \\\\ \\pi^u(\\sigma_{ij}) \u0026amp;\u0026gt; \\frac{1}{2\\tau \\sqrt{2\\pi^3}}\\log\\Big(1+\\frac{4\\tau^2}{\\sigma_{ij}^2} \\Big) \\end{aligned}\\end{equation*}$$\nWe will deal with $$\\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j}\\pi^u(\\abs{\\sigma_{ij}}\\leq \\sqrt \\phi)$$ first.\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\pi^u(\\abs{\\sigma_{ij}}\u0026gt;\\sqrt \\phi) = 2\\pi^u(\\sigma_{ij}\u0026gt;\\sqrt \\phi) = \\int_{\\sqrt\\phi}^\\infty \\pi^u(\\sigma_{ij}), d\\sigma_{ij}\\\\ \u0026amp;\\leq 2\\int_{\\sqrt\\phi}^\\infty \\frac{1}{\\tau \\sqrt{2\\pi^3}}\\log\\Big(1+\\frac{2\\tau^2}{\\sigma_{ij}^2} \\Big) ,d\\sigma_{ij} \\quad \\because \\; \\text{upper bound of marginal prior density of $\\sigma_{ij}$} \\\\ \u0026amp;\\leq 2\\int_{\\sqrt \\phi}^\\infty \\frac{1}{\\tau \\sqrt{2\\pi^3}} \\frac{2\\tau^2}{\\sigma_{ij}^2} , d\\sigma_{ij} \\quad \\because \\; \\log(1+x)\\leq x \\quad \\forall \\; x\u0026gt;-1 \\quad \\text{by supporting line lemma}\\ \u0026amp;= 2\\tau \\sqrt{\\frac{2}{\\pi^3}} \\int_{\\sqrt{\\phi}}^\\infty \\frac{1}{\\sigma_{ij}^2}, d\\sigma_{ij} = 2\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\\\ \u0026amp;\\\\ \u0026amp;\\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j}\\pi^u(\\abs{\\sigma_{ij}}\\leq \\sqrt \\phi) = \\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j} \\big(1 - \\pi^u(\\abs{\\sigma_{ij}}\u0026gt;\\sqrt{\\phi})\\big) \\\\ \u0026amp;\\geq \\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j} \\Big(1- 2\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big) \\quad \\because \\; \\pi^u(\\abs{\\sigma_{ij}}\u0026gt;\\sqrt \\phi) \\leq 2\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\\\ \u0026amp;\\geq \\Big(1- 2\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big)^{p^2} \\\\ \u0026amp;\\geq \\exp\\Big(-4\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big)^{p^2} \\quad \\because \\; \\log(1-x)\\geq -2x \\quad \\text{when}\\; 0\\leq x\\leq 1/2 \\\\ \u0026amp;\\text{Note that} \\; \\tau/\\sqrt{\\phi} \\; \\text{is small enough when $n$ is sufficiently large} \\quad \\because\\; (p^2\\sqrt{n})^{-1}\\lesssim \\tau \\lesssim (p^2\\sqrt{n})^{-1}\\sqrt{s_0\\log p} \\\\ \u0026amp;\\tau/\\sqrt{\\phi} \\leq C \\frac{1}{p^2}\\sqrt{\\frac{s_0\\log p}{n}}\\sqrt{\\frac{p(p-1)n}{s_0\\log p}}\\frac{2}{3\\zeta_0^2\\zeta^4} \\leq \\tilde C \\frac{1}{p} \\rightarrow 0 \\end{aligned}\\end{equation*}$$\nWe can proceed the inequality as the following. $$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{(i,j) \\notin s(\\Sigma_0), ,, i\u0026lt;j}\\pi^u(\\abs{\\sigma_{ij}}\\leq \\sqrt \\phi) \\geq \\exp\\Big(-4\\tau \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big)^{p^2} \\\\ \u0026amp;= \\exp\\Big(-4\\tau p^2 \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big) \\\\ \u0026amp;\\geq \\exp\\Big(-4\\sqrt{\\frac{2}{\\pi^3}}\\tilde C p^2\\frac 1p \\Big) \\quad \\because \\; \\tau/\\sqrt{\\phi}\\leq \\tilde C \\frac 1p \\quad \\text{by above} \\\\ \u0026amp;= \\exp(-Cp) \\quad \\text{for some } C\u0026gt;0 \\end{aligned}\\end{equation*}$$\nThus we have $$ \\begin{equation} \\label{offdiagonal zeros ineq} \\prod _ {(i,j) \\notin s(\\Sigma _ 0)\\, ,\\, i \u0026lt; j } \\pi ^ u(\\abs{\\sigma _ {ij}}\\leq \\sqrt \\phi) \\geq \\exp\\Big(-4\\tau p ^ 2 \\sqrt{\\frac{2}{\\pi^3}} \\frac{1}{\\sqrt{\\phi}} \\Big) \\geq \\exp(- Cp) \\end{equation} $$ for sufficiently large $n$.\nFinally, we shall find the lower bound of $$\\prod _ {(i,j)\\in s(\\Sigma _ 0)}\\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi)$$\nRecall that marginal prior density of off diagonal $\\sigma_{ij}$ given as $\\pi^u(\\sigma_{ij})$ is decreasing function with respect to $\\abs{\\sigma_{ij}}$ since $$ \\pi^u(\\sigma_{ij})= \\int_0^\\infty \\frac{1}{\\sqrt{2\\pi^3 \\tau^2}} \\exp\\Big(-\\frac{\\sigma_{ij}^2}{2\\tau^2} u \\Big) \\frac{1}{1+u} , du $$ Also, note that since $\\phi = \\frac{2}{3\\zeta^4\\zeta_0^2}\\frac{s_0\\log p}{p(p-1)n}\\rightarrow 0 $ as $n$ tends to sufficiently large, we can write $$\\begin{equation*} (\\abs{\\sigma_{ij}- \\sigma_{ij}^\\ast}\\leq \\sqrt\\phi) = (\\sigma_{ij}^\\ast - \\sqrt\\phi \\leq \\sigma_{ij} \\leq \\sigma_{ij}^\\ast + \\sqrt \\phi) \\begin{cases}\\subset (0, \\infty) \u0026amp; \\; \\text{if} \\quad \\sigma_{ij}^\\ast \u0026gt;0 \\\\ \\subset (-\\infty, 0) \u0026amp;\\; \\text{if} \\quad \\sigma_{ij}^\\ast \u0026lt;0 \\end{cases} \\end{equation*}$$ for sufficiently large $n$, since $\\sigma_{ij}^\\ast \\neq 0 \\; \\text{due to} \\; (i, j)\\in s(\\Sigma_0)$\nTherefore, we have the following inequality. $$\\begin{equation*}\\begin{aligned} \\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi) \\begin{cases} \\geq 2\\sqrt{\\phi} , \\pi^u(\\sigma_{ij}^\\ast + \\sqrt{\\phi}) \u0026amp; \\quad \\text{if} \\quad \\sigma_{ij}^\\ast \u0026gt;0 \\\\ \\geq 2\\sqrt{\\phi} , \\pi^u(\\sigma_{ij}^\\ast - \\sqrt{\\phi}) \u0026amp; \\quad \\text{if} \\quad \\sigma_{ij}^\\ast \u0026lt;0 \\end{cases} \\end{aligned}\\end{equation*}$$\nCombining three facts, we can yield $\\abs{\\sigma_{ij}^\\ast} \\leq \\zeta_0$ for all $i\\neq j$. Those facts are given as the following.\nThe largest entry in magnitude of positive definite matrix lies on the diagonal (Source : Gockenbagh Linear Algebra Lemma 386) Energy boundedness : $\\lambda_n \\norm{x}_2^2 \\leq x^T A x \\leq \\lambda_1 \\norm{x}_2^2$ if symmetric $A$ has $\\text{spec}(A) = \\{\\lambda_1\\geq \\cdots \\geq \\lambda_n\\}$ $\\lambda_{max}(\\Sigma_0)\\leq \\zeta_0$ by $\\Sigma_0 \\in \\Uc(s_0, \\zeta_0)$ assumption Hence we have $$\\begin{equation*}\\begin{aligned} \\abs{\\sigma_{ij}^\\ast} \\leq \\max_k{\\abs{\\sigma_{kk}^\\ast}} = \\max_k \\sigma_{kk}^\\ast \\leq \\lambda_{max}(\\Sigma_0) \\leq \\zeta_0 \\end{aligned}\\end{equation*}$$ and what follow this are $$\\begin{equation*}\\begin{aligned} \u0026amp;\\sigma_{ij}^\\ast + \\sqrt{\\phi} \\leq 2\\zeta_0 \\quad \\text{if} \\quad \\sigma_{ij}^\\ast \u0026gt;0 \\\\ \u0026amp;\\abs{\\sigma_{ij}^\\ast - \\sqrt{\\phi}} \\leq 2\\zeta_0 \\quad \\text{if} \\quad \\sigma_{ij}^\\ast \u0026lt; 0 \\end{aligned}\\end{equation*}$$\nUsing this, we get $\\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi) \\geq 2\\sqrt{\\phi}, \\pi^u(2\\zeta_0)$\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{(i,j)\\in s(\\Sigma_0)}\\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi) \\geq \\Big(2\\sqrt{\\phi}, \\pi^u(2\\zeta_0) \\Big)^{s_0} \\geq \\Big(\\sqrt{\\phi}, \\pi^u(2\\zeta_0) \\Big)^{s_0} \\geq \\Bigg(\\pi^u(2\\zeta_0)\\sqrt{\\frac{2s_0\\log p}{3\\zeta^4 \\zeta_0^2 p^2 n}} \\Bigg)^{s_0} \\\\ \u0026amp;= \\exp\\Bigg(s_0\\log \\pi^u(2\\zeta_0) + \\frac 12 s_0 \\log \\frac{2s_0\\log p}{3\\zeta^4\\zeta_0^2p^2 n} \\Bigg) \\\\ \u0026amp;\\geq \\exp\\Big(s_0 \\log \\pi^u(2\\zeta_0) + \\frac 12 s_0 \\log \\frac 23\\frac{1}{\\zeta^2p^2n} \\Big) \\quad \\because\\; \\zeta^2 \\zeta_0^2 \\leq s_0\\log p \\quad \\text{by assumption} \\end{aligned}\\end{equation*}$$\nNote that by taking advantage of Lemma 4, we can write $\\pi^u(2\\zeta_0)\\geq \\frac{1}{\\sqrt{2\\pi^3}}\\frac{\\tau}{4\\zeta_0^2} $ Of course, we should show that $\\tau/2\\zeta_0$ is sufficiently small to justify the use of Lemma 4. Since $\\zeta_0$ is fixed, we shall show that $\\tau \\rightarrow 0$ as $n\\rightarrow \\infty$\n$$\\begin{equation*}\\begin{aligned} \\tau \u0026amp; \\lesssim \\frac{\\sqrt{s_0\\log p}}{p^2\\sqrt{n}} \\quad \\text{by assumption} \\\\ \u0026amp; \\lesssim \\frac{\\sqrt{s_0\\log s_0}}{p^2\\sqrt{n}} \\quad \\because p \\lesssim s_0 \\\\ \u0026amp;\\leq \\frac{s_0}{p^2\\sqrt{n}} \\leq \\frac{1}{\\sqrt{n}} \\quad \\because \\; \\log s_0 \\leq s_0 \\leq p^2 \\end{aligned}\\end{equation*}$$\nThus $\\tau \\lesssim \\frac{1}{\\sqrt{n}}$ so that $\\tau / \\zeta_0$ is sufficiently small as $n$ gets sufficiently large.\nCombining with the condition $\\tau \\gtrsim 1/\\sqrt{n}p^2$ , we get $\\pi^u(2\\zeta_0)\\geq \\frac{1}{\\sqrt{2\\pi^3}}\\frac{\\tau}{4\\zeta_0^2} \\gtrsim \\frac{1}{\\sqrt{n}p^2} $\nNow we shall proceed our target inequality.\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod_{(i,j)\\in s(\\Sigma_0)}\\pi^u(\\abs{\\sigma_{ij} - \\sigma_{ij}^\\ast}\\leq \\sqrt \\phi)\\ \u0026amp;\\geq \\exp\\Big(s_0 \\log \\pi^u(2\\zeta_0) + \\frac 12 s_0 \\log \\frac 23\\frac{1}{\\zeta^2p^2n} \\Big) \\\\ \u0026amp;\\geq \\exp\\Big(s_0 \\log \\Big(\\frac{\\tilde C}{\\sqrt{n}p^2} \\Big) + \\frac 12 s_0 \\log \\frac 23\\frac{1}{\\zeta^2p^2n} \\Big) \\\\ \u0026amp;= \\exp\\Big(\\frac 12 s_0 \\log \\Big(\\frac{\\tilde C^2}{np^4} \\Big) + \\frac 12 s_0 \\log \\frac 23\\frac{1}{\\zeta^2p^2n} \\Big) \\\\ \u0026amp;= \\exp \\Big(\\frac 12 s_0 \\log \\big(\\frac{2\\tilde{C}^2}{3\\zeta^2p^6n^2} \\big)\\Big) \\end{aligned}\\end{equation*}$$\nHere, we gonna use two inequalities\n$C^\\ast p^{-2/\\beta} \\leq n^{-2}$ for some $C^\\ast\u0026gt;0$ $1/\\zeta^2 \\geq 1/p$ The first one comes from $p\\asymp n^\\beta$ so that $n^\\beta \\leq \\tilde C^{\\ast} p$ and the second one comes from $\\zeta^4\\leq p$ and $1\u0026lt;\\zeta_0 \u0026lt; \\zeta$. Using those inequalities, we get\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\prod _ {(i,j)\\in s(\\Sigma _ 0)}\\pi^u(\\abs{\\sigma _ {ij} - \\sigma _ {ij}^\\ast}\\leq \\sqrt \\phi)\\\\ \u0026amp;\\geq \\exp \\Big(\\frac 12 s _ 0 \\log \\big(\\frac{2\\tilde{C}^2}{3\\zeta^2p^6n^2} \\big)\\Big) \\\\ \u0026amp;\\geq \\exp\\Big(\\frac 12 s _ 0 \\log\\big(\\frac 23 \\tilde C^2 p^{-1}p^{-6} C^\\ast p^{-2/\\beta} \\big) \\Big) \\\\ \u0026amp;= \\exp\\Big(\\frac 12 s _ 0 \\log (Cp^{-(7+\\frac{2}{\\beta})}) \\Big) \\\\ \u0026amp;= \\exp\\Big(-\\frac12(7+\\frac 2\\beta)s _ 0 \\log p + \\frac 12 s _ 0 \\log C \\Big) \\\\ \u0026amp;\\geq \\exp\\Big(-\\frac12(7+\\frac 2\\beta)s _ 0 \\log p - \\frac 12 s _ 0 \\log p \\Big) \\quad \\text{for suff. large } n \\\\ \u0026amp;= \\exp\\Big(-(4+\\frac 1\\beta)s _ 0\\log p \\Big) \\end{aligned}\\end{equation*}$$\nHence we have\n$$ \\begin{equation}\\label{offdiagonal nonzeros ineq} \\prod _ { (i,j) \\in s( \\Sigma _ 0 ) } \\pi ^ u( \\abs{ \\sigma _ {ij} - \\sigma _ {ij}^\\ast} \\leq \\sqrt \\phi ) \\geq \\exp \\left (- (4+ \\frac 1 \\beta ) s _ 0 \\log p \\right ) \\end{equation} $$\nAt last, combining \\eqref{diagonal ineq}, \\eqref{offdiagonal zeros ineq}, and \\eqref{offdiagonal nonzeros ineq}, we have\n$$\\begin{equation*}\\begin{aligned} \u0026amp;\\pi^u(A_{n, \\Sigma_0}) \\geq \\exp\\Big(-(3+\\frac{1}{2\\beta})p \\log p\\Big) \\times \\exp(-Cp) \\times \\exp\\Big(-(4+\\frac 1\\beta)s_0\\log p \\Big) \\\\ \u0026amp;\\geq \\exp\\Big(-(3+\\frac{1}{2\\beta})p \\log p\\Big) \\times \\exp(-Cp) \\times \\exp\\Big(-(4+\\frac 1\\beta)s_0\\log p \\Big) \\times \\exp(Cp) \\times \\exp(-p\\log p) \\\\ \u0026amp;= \\exp\\Big(-(4+\\frac{1}{2\\beta})p \\log p\\Big) \\times \\exp\\Big(-(4+\\frac 1\\beta)s_0\\log p \\Big) \\geq \\exp\\Big(-(4+\\frac 1\\beta)(p+s_0)\\log p\\Big) \\\\ \u0026amp;= \\exp\\Big(-(4+\\frac 1\\beta)n\\eps_n^2 \\Big) \\end{aligned}\\end{equation*}$$\nSince we have already shown that\n$$\\pi(B _ {\\eps _ n}) \\geq \\pi\\Big(\\norm{\\Sigma- \\Sigma _ 0} _ F^2 \\leq \\frac{2}{3\\zeta^4\\zeta _ 0^2}\\eps _ n^2\\Big) \\geq \\pi(A _ {n, \\Sigma _ 0})\\geq \\pi^u(A _ {n, \\Sigma _ 0})$$\nwe can conclude that\n$$\\pi(B_{\\eps_n}) \\geq \\exp\\Big(-(4+\\frac 1\\beta) n\\eps_n^2 \\Big) $$\n"},{"id":5,"href":"/2022-Q2-workshop/docs/06/","title":"Theorem 2 ","section":"2022 Q2 Bayes Workshop","content":" 모형 # 다음의 모형을 생각하자.\n$$ \\begin{equation}\\label{eqn-model} X_1, \\cdots, X_n | \\Sigma \\sim N(0, \\Sigma) \\end{equation} $$\n양의 정수 $s_0$와 실수 $\\zeta_0 \u0026gt; 1$에 대해 다음과 같은 모수공간을 생각한다. \\begin{equation} U(s_0, \\zeta_0) = \\{ \\Sigma \\in C_p: s(\\Sigma) \\leq s_0,~ \\zeta_0^{-1} \\leq \\lambda_{\\min}(\\Sigma) \\leq \\lambda_{\\max}(\\Sigma) \\leq \\zeta_0 \\} \\end{equation} 여기서 $s(\\Sigma)$는 행렬 $\\Sigma$의 0이 아닌 비대각성분의 개수를 의미한다.\n정리 # 모형 (1)과 양의 정수 $s_0$ 와 실수 $\\zeta_0 \u003e 1$에 대해 $\\Sigma_0 \\in \\mathcal{U}(s_0, \\zeta_0)$이라 하자. $s_0^2 (\\log p)^3 = O(p^2n)$이면 작은 상수 $\\epsilon \u003e 0$에 대해 다음이 성립한다. \\begin{equation} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in \\mathcal{U}(s_0, \\zeta_0)} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{(p+s_0) \\log p}{n} I\\left(3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}\\right) + \\frac{p+s_0}{n} \\end{equation}\n이 정리는 공분산 추정의 minimax lower bound를 알려준다.\n논문의 Theorem 1에서는 사후수렴속도가 $\\dfrac{(p+s_0) \\log p}{n}$임을 보였는데, 이는 $3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}$일 때 베이즈 추론이 minimax 이고, 그렇지 않은 경우에도 nearly minimax $(\\log p)$ 임을 의미한다.\n이와 관련된 연구로 Cai와 Zhou (2012)1가 있는데, 이 논문에서는 빈도론 관점에서 성긴 공분산 행렬을 추론하는 문제를 다루었다. 다만, 논문에서는 공분산 행렬의 각 열의 0이 아닌 성분에 대한 제약조건을 다루었으나, 본 논문에서는 전체 행렬에서 0이 아닌 성분에 대한 제약조건에 대해 다룬다.\n증명 # 다음의 두 항목을 증명하면 된다. $3p \u0026lt; s_0 \u0026lt; p^{3/2 - \\epsilon/2}$인 경우, $$\\begin{equation}\\label{eqn-13} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in B_1} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{s_0 \\log p}{n} \\end{equation}$$ 이 성립하는 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$가 존재함을 보인다. (이경원, 정진욱) 나머지 경우, $$\\begin{equation}\\label{eqn-14} \\inf_{\\hat{\\Sigma}}\\sup_{\\Sigma_0 \\in B_2} \\mathbb{E}_0 \\lVert\\hat{\\Sigma} - \\Sigma_0 \\rVert_F^2 \\gtrsim \\frac{s_0 + p}{n} \\end{equation}$$ 이 성립하는 $B_2 \\subset \\mathcal{U}(s_0, \\zeta_0)$가 존재함을 보인다. (김성민) 먼저 첫 항목을 보이자. $\\nu = \\sqrt{\\epsilon/4}$에 대해 $r = \\lfloor p/2 \\rfloor,~\\epsilon_{np} = \\nu \\sqrt{\\log p / n}$이라 하자. $A_m(u)$를 $m$번째 행과 열이 $u$의 값을 갖고, 나머지에서 모두 0인 대칭행렬이라 하자. 즉, \\begin{equation} (A_m(u))_{ij} = \\begin{cases} u \u0026amp; i = m \\text{ or} j = m \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\end{equation}\n이라 하자. 이제, 다음과 같이 $B_1$을 정의한다.\n\\begin{equation} B_1 := \\left\\{ \\Sigma(\\theta) : \\Sigma(\\theta) = I_p + \\epsilon_{np} \\sum_{m=1}^r \\gamma_m A_m(\\lambda_m),~\\theta = (\\gamma, \\lambda) \\in \\Theta \\right\\} \\end{equation}\n여기서 $\\gamma = (\\gamma_1,\\cdots, \\gamma_r) \\in \\Gamma = \\{0, 1\\}^r$, $\\lambda = (\\lambda_1, \\cdots, \\lambda_r)^T \\in \\Lambda \\subset \\mathbb{R}^{r \\times p}$, \\begin{equation} \\begin{aligned} \\Lambda= \\bigg\\{ \\lambda = (\\lambda_{ij}) : \u0026amp;\\lambda_{mi} \\in \\{0, 1\\},~ \\lVert \\lambda_m \\rVert_0 = k,~\\sum_{i=1}^{p-r} \\lambda_{mi} = 0, \\\\ \u0026amp;m \\in \\{ 1, \\cdots, r \\},~ \\text{ satisfying } \\max_{1 \\leq i \\leq p} \\sum_{m=1}^r \\lambda_{mi} \\leq 2k \\bigg\\}, \\end{aligned} \\end{equation} $k = \\lceil c_{np} / 2 \\rceil - 1,~ c_{np} = \\lceil s_0 / p \\rceil$, $\\Theta = \\Gamma \\times \\Lambda$이다.\n이제 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$과 논문의 식 (13)이 성립함을 보이면 된다.\n먼저, 임의의 $\\zeta_0 \u0026gt;1$과 충분히 큰 $n$에 대해 $\\Sigma(\\theta) \\in B_1$의 가장 큰 고유치가 $\\zeta_0$보다 작다는 것은 다음과 같이 보일 수 있다. \\begin{equation} \\lambda_{\\max}\\left(\\Sigma(\\theta)\\right) \\leq \\lVert \\Sigma(\\theta) \\rVert_1 \\leq 1 + 2 k \\epsilon_{np} \\leq 1 + c_{np} \\nu \\sqrt{\\log p / n} \\leq \\zeta_0 \\end{equation} 두 번째 부등호는 모수공간 $\\Lambda$의 마지막 조건으로부터, 마지막 부등호는 가정 $s_0^2 (\\log p)^3 = O(p^2 n)$에 의해 성립한다.\n마찬가지의 이유로 임의의 $\\zeta_0 \u0026gt;1$과 충분히 큰 $n$에 대해 \\begin{equation} 2k\\epsilon_{np} \\leq c_{np} \\nu \\sqrt{\\log p / n}\\leq \\left( 1 + \\frac{s_0}{p} \\right) \\nu \\sqrt{\\log p / n} \\leq 1 - \\zeta_0^{-1} \\end{equation} 가 성립하므로 $\\Sigma(\\theta) - \\zeta_0^{-1} I_p$는 대각지배(diagonally dominant)행렬이고, 대칭이며 모든 성분이 0보다 크거나 같아 양의 준정부호 행렬이다2. 따라서, $\\Sigma(\\theta)$의 가장 작은 고유치는 $\\zeta_0^{-1}$보다 크다.\n마지막으로 $\\Sigma(\\theta)$의 비대각성분은 모두 $A_m$들에 의해서만 나타나므로 \\begin{equation} s(\\Sigma(\\theta)) \\leq 2 kp \\leq s_0 \\end{equation} 에서 $B_1 \\subset \\mathcal{U}(s_0, \\zeta_0)$를 얻는다.\n이제 논문의 식 (13)이 성립함을 보이자. 이를 위해, 다음의 보조정리를 소개한다. 이 보조정리는 모수공간 $\\Theta$에서 거리 $d$를 갖는 거리공간으로의 변환 $\\psi(\\theta)$의 최대 위험의 하한을 알려준다.\n(Lemma 3 of Cai and Zhou (2012)) OPTIMAL RATES OF CONVERGENCE FOR SPARSE COVARIANCE MATRIX ESTIMATION\nFor any $s \u0026gt; 0$ and any estimator $T$ of $\\psi(\\theta)$ based on an observation from the experiment $\\{ P_\\theta,~\\theta \\in \\Theta \\}$,\n\\begin{equation} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \\geq \\alpha \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert, \\end{equation}\nwhere $\\overline{\\mathbb{P}} _ {i, a}$ is the mixture distribution over all $P_\\theta$ with $\\gamma_i(\\theta)$ ﬁxed to be a while all other components of $\\theta$ vary over all possible values, i.e., \\begin{equation} \\overline{\\mathbb{P}} _ {i, a} = \\frac{1}{2^{r-1} |\\Lambda|} \\sum_{\\theta \\in \\Theta_{i, a}} P_\\theta, \\end{equation}\nfor $\\Theta_{i, a} = \\{ \\theta \\in \\Theta: \\gamma_i(\\theta) = a \\}$,\n\\begin{equation} \\lVert \\mathbb{P} \\wedge \\mathbb{Q} \\rVert = \\int (p \\wedge q) d \\mu, \\end{equation} for probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ which have densities $p$ and $q$ respectively, $\\mathbb{E} _ \\theta$ is expectation with respect to $[X_1, \\cdots, X_n | \\theta]$, $H(x, y)$ is the Hamming distance defined as $$H(x, y) = \\sum_{j=1}^r |x_i - y_i|, \\quad x, y \\in \\{ 0, 1 \\}^r.$$\n\\begin{equation} \\alpha = \\min_{(\\theta, \\theta^\\prime) : H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\geq 1} d^s(\\psi(\\theta), \\psi(\\theta^\\prime)) / H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{equation}\n최댓값은 평균보다 크거나 같으므로 \\begin{equation} \\begin{aligned} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} 2^s \\mathbb{E} _ \\theta d^s(T, \\psi(\\theta)) \\\\ \u0026amp;= \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta (2 d(T, \\psi(\\theta)))^s \\end{aligned} \\end{equation}\n$\\hat{\\theta} := \\arg\\min d^s(T, \\psi(\\theta))$라 하면 (유일하지 않다면 적당히 하나를 잡으면 된다.)\n\\begin{equation} \\begin{aligned} \\mathbb{E} _ \\theta (2 d(T, \\psi(\\theta)))^s \u0026amp;\\geq \\mathbb{E} _ \\theta (d(T, \\psi(\\theta)) + d(T, \\psi(\\hat{\\theta})) )^s \\\\ \u0026amp;\\geq \\mathbb{E} _ \\theta (d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s \\end{aligned} \\end{equation} 를 얻는다. 마지막 부등식에서 삼각부등식을 사용하였다.\n정리하면 다음을 얻는다.\n\\begin{equation} \\begin{aligned} \\max_{\\theta \\in \\Theta} 2^s \\mathbb{E}_\\theta d^s(T, \\psi(\\theta)) \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta (d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s \\\\ \u0026amp;\\geq \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[\\frac{(d(\\psi(\\hat{\\theta}), \\psi(\\theta)))^s}{H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\vee 1 } H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\\\ \u0026amp;\\geq \\alpha \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\end{aligned} \\end{equation}\n이제 \\begin{equation} \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\geq \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{equation} 을 보이면 원하는 결과를 얻는다.\n\\begin{equation} \\begin{aligned} \u0026amp;\\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\mathbb{E} _ \\theta \\left[ H(\\gamma(\\theta), \\gamma(\\theta^\\prime))\\right] \\\\ \u0026amp;= \\frac{1}{2^r |\\Lambda| } \\sum _ {\\theta \\in \\Theta} \\sum_{i=1}^r \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\\\ \u0026amp;= \\sum_{i=1}^r \\frac{1}{2^r |\\Lambda| } \\sum_{\\rho \\in \\Gamma} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta) - \\gamma_i(\\theta^\\prime))| \\right] \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |\\gamma_i(\\theta^\\prime))| \\right] + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\mathbb{E} _ \\theta \\left[ |1 - \\gamma_i(\\theta^\\prime))| \\right] \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) d\\mathbb{P}_ {\\theta^\\prime} + \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} \\int _ {\\theta^\\prime} 1 - \\gamma_i(\\theta^\\prime)) d\\mathbb{P}_ {\\theta^\\prime} \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 0} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} d\\mathbb{P}_ {\\theta^\\prime} + \\int _ {\\theta^\\prime} \\left( 1 - \\gamma_i(\\theta^\\prime)) \\right) \\frac{1}{2^{r-1} |\\Lambda| } \\sum_{\\rho_i = 1} \\sum _ {\\theta : \\gamma(\\theta) = \\rho} d\\mathbb{P}_ {\\theta^\\prime} \\right\\} \\\\ \u0026amp;= \\frac{1}{2} \\sum_{i=1}^r \\left\\{ \\int _ {\\theta^\\prime} \\gamma_i(\\theta^\\prime)) d \\overline{\\mathbb{P}} _ {i, 0} + \\int _ {\\theta^\\prime} \\left( 1 - \\gamma_i(\\theta^\\prime)) \\right) d \\overline{\\mathbb{P}} _ {i, 0} \\right\\} \\\\ \u0026amp;\\geq \\frac{1}{2} \\sum_{i=1}^r \\int d \\left[ \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\right] \\\\ \u0026amp;\\geq \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{aligned} \\end{equation}\n증명이 끝났다.\n위의 보조정리에 $s=2$를 대입하면 다음의 부등식을 얻는다. \\begin{equation} \\inf_{\\hat{\\Sigma}} \\max_{\\theta \\in \\Theta} 2^2 \\mathbb{E}_\\theta \\lVert \\hat{\\Sigma}- \\Sigma(\\theta) \\rVert_F^2 \\geq \\alpha \\frac{r}{2} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\end{equation}\n여기서 \\begin{equation} \\alpha = \\min_{(\\theta, \\theta^\\prime) : H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\geq 1} \\lVert \\Sigma(\\theta) - \\Sigma(\\theta^\\prime) \\rVert_F^2 / H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{equation} 이다.\n이때 임의의 $\\theta,~\\theta^\\prime \\in \\Theta$에 대해 \\begin{equation} \\begin{aligned} \\lVert \\Sigma(\\theta) - \\Sigma(\\theta^\\prime) \\rVert_F^2 \u0026amp;= \\epsilon_{np}^2 \\left\\lVert \\sum_{m=1}^r \\gamma_m(\\theta) A_m(\\lambda_m(\\theta)) - \\sum_{m=1}^r \\gamma_m(\\theta^\\prime) A_m(\\lambda_m(\\theta^\\prime)) \\right\\rVert_F^2 \\\\ \u0026amp; \\geq 2k\\epsilon_{np}^2 H(\\gamma(\\theta), \\gamma(\\theta^\\prime)) \\end{aligned} \\end{equation} 이므로 $k$와 $r$의 정의($r = \\lfloor p/2 \\rfloor,~k = \\lceil c_{np} / 2 \\rceil - 1,~ c_{np} = \\lceil s_0 / p \\rceil$)로부터 다음을 얻는다.\n\\begin{equation} \\alpha r \\geq 2k\\epsilon_{np}^2 r \\geq \\nu^2 \\left( \\frac{1}{2} - \\frac{p}{s_0} \\right) \\frac{s_0 \\log p}{n} \\asymp \\frac{s_0 \\log p}{n} \\end{equation}\n이제, 다음을 만족하는 적당한 상수 $c_1 \u0026gt; 0$이 존재함을 보이면 증명이 끝난다. \\begin{equation} \\min _ {1 \\leq i \\leq r} \\lVert \\overline{\\mathbb{P}} _ {i, 0} \\wedge \\overline{\\mathbb{P}} _ {i, 1} \\rVert \\geq c_1 \\end{equation}\nTBA\nT.T. Cai, H.H. Zhou, Optimal rates of convergence for sparse covariance matrix estimation, Ann. Statist. 40 (5) (2012) 2389–2420.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Hermitian diagonally dominant matrix $A$ with real non-negative diagonal entries is positive semidefinite. From https://en.wikipedia.org/wiki/Diagonally_dominant_matrix#Applications_and_properties 혹은, 더 간단하게 Gershgorin circle theorem에 symmetric matrix가 real eigenvalue를 가진다는 사실로도 보일 수 있다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]